<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><link rel="shortcut icon" type="image/x-icon" href="/bigdata-lecture/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>통계학 기초 | TensorFlow로 시작하는 머신러닝 &amp; 딥러닝</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="통계학 기초" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="머신러닝에 반드시 필요한 통계학의 기초 이론을 배웁니다." />
<meta property="og:description" content="머신러닝에 반드시 필요한 통계학의 기초 이론을 배웁니다." />
<link rel="canonical" href="https://gusdnd852.github.io/bigdata-lecture/statics_theory" />
<meta property="og:url" content="https://gusdnd852.github.io/bigdata-lecture/statics_theory" />
<meta property="og:site_name" content="TensorFlow로 시작하는 머신러닝 &amp; 딥러닝" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-07-20T00:00:00-05:00" />
<script type="application/ld+json">
{"url":"https://gusdnd852.github.io/bigdata-lecture/statics_theory","@type":"BlogPosting","headline":"통계학 기초","dateModified":"2020-07-20T00:00:00-05:00","datePublished":"2020-07-20T00:00:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://gusdnd852.github.io/bigdata-lecture/statics_theory"},"description":"머신러닝에 반드시 필요한 통계학의 기초 이론을 배웁니다.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  <link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
  <link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css">
    <link rel="stylesheet" href="/bigdata-lecture/assets/main.css"><link type="application/atom+xml" rel="alternate" href="https://gusdnd852.github.io/bigdata-lecture/feed.xml" title="TensorFlow로 시작하는 머신러닝 & 딥러닝" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
      document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
          delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
          ]}
        );
      });
    </script>
  

  <script>
  function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
  }
  window.onload = wrap_img;
  </script>

  <script>
    document.addEventListener("DOMContentLoaded", function(){
      // add link icon to anchor tags
      var elem = document.querySelectorAll(".anchor-link")
      elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
      // remove paragraph tags in rendered toc (happens from notebooks)
      var toctags = document.querySelectorAll(".toc-entry")
      toctags.forEach(e => (e.firstElementChild.innerText = e.firstElementChild.innerText.replace('¶', '')))
    });
  </script>
</head>
<meta name="viewport" content="width=device-width, initial-scale=1.0">
    <style>

     @media screen and (max-width : 511px) {
         body { 
		font-size: 0.84rem; 
	}pre, code, blockquote {
		font-size: 0.84rem !important;
	}
	.katex, .mord {
		font: normal 1.02em 'KaTeX_Main', sans-serif !important;
	}
	.minner, .mord.mtight {
		font: normal 0.90em 'KaTeX_Main', sans-serif !important;
	}
	.page-meta {
		font-size: 0.9rem !important;
	}
	.contents-left{
		display: none !important;
	}
	.contents-right, .contents-left {
    		display: block;
    		text-align: center;
		margin-left: 0px !important;
    		margin-right: 0px !important;
    		list-style: none;
    	}
	.mobile_hide{
		display: none;
	}
	.contents-right > li {
    		display: inline-block;
    		margin: 3px;
    		word-break: keep-all;
	}
     }
 

     @media screen and (min-width : 512px) and (max-width : 767px) {
         body { 
		font-size: 0.92rem; 
	}pre, code, blockquote {
		font-size: 0.92rem !important;
	}
	.katex, .mord {
		font: normal 1.04em 'KaTeX_Main', sans-serif !important;
	}
	.minner, .mord.mtight {
		font: normal 0.92em 'KaTeX_Main', sans-serif !important;
	}
	.page-meta {
		font-size: 1.0rem !important;
	}
	.contents-left{
		display: none !important;
	}
	.contents-right, .contents-left {
    		display: block;
    		text-align: center;
		margin-left: 0px !important;
    		margin-right: 0px !important;
    		list-style: none;
    	}    
	.mobile_hide{
		display: none !important;
	}
	.contents-right > li {
    		display: inline-block;
    		margin: 3px;
    		word-break: keep-all;
	}
     }
 
     @media screen and (min-width : 768px) and (max-width : 1024px) {
         body {
		 font-size: 1.0rem; 
	}pre, code, blockquote {
		font-size: 1.0rem !important;
	}
	.katex, .mord {
		font: normal 1.05em 'KaTeX_Main', sans-serif !important;
	}
	.minner, .mord.mtight {
		font: normal 0.93em 'KaTeX_Main', sans-serif !important;
	}
	.page-meta {
		font-size: 1.1rem !important;
	}
	.contents-right, .contents-left {
		display: inline-block;
    		vertical-align: top;
    		text-align: left;
		list-style: none;
    	}  
	.contents-left{
		margin-left: 20px !important;
    		margin-right: 3.0rem;
	}
	.mobile_hide{
		display: block !important;
	}
     }

     @media screen and (min-width : 1025px) {
         body { 
		font-size: 1.08rem; 
	}pre, code, blockquote {
		font-size: 1.08rem !important;
	}
     	.katex, .mord {
		font: normal 1.06em 'KaTeX_Main', sans-serif !important;
	}
	.minner, .mord.mtight {
		font: normal 0.94em 'KaTeX_Main', sans-serif !important;
	}
	.page-meta {
		font-size: 1.2rem !important;
	}
	.contents-right, .contents-left {
    		display: inline-block;
    		vertical-align: top;
    		text-align: left;
		list-style: none;
    	}
	.contents-right > h2, .contents-left > h2 {
    		margin-botton: 2px
	}  
	.contents-left{
    		margin-left: 0 !important;
    		margin-right: 3.0rem;
	}
	.mobile_hide{
		display: block !important;
	}
     }

    </style>
  <body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" style="font-size:1.2rem;" href="/bigdata-lecture/">TensorFlow로 시작하는 머신러닝 &amp; 딥러닝 </a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/bigdata-lecture/lecture">Lecture</a><a class="page-link" href="/bigdata-lecture/orientation">Orientation</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content" style="flex: none;" >
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h3 class="post-title p-name" itemprop="name headline">03. 통계학 기초</h3><p class="page-description">머신러닝에 반드시 필요한 통계학의 기초 이론을 배웁니다.</p
      <i class="fas fa-tags category-tags-icon"></i><p class="category-tags"> 
      
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
	
	<div class="px-1">
    <a href="https://colab.research.google.com/github/gusdnd852/bigdata-lecture/blob/master/_notebooks/02_03_통계학_기초.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/bigdata-lecture/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
	
	<div class="px-1">
<a href="https://github.com/gusdnd852/bigdata-lecture/tree/master/_notebooks/02_03_통계학_기초.ipynb" role="button">
    <img class="notebook-badge-image" src="https://img.shields.io/static/v1?label=&message=View%20On%20GitHub&color=586069&logo=github&labelColor=2f363d">
</a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h3"><a href="#1.-통계학이란?">1. 통계학이란? </a></li>
<li class="toc-entry toc-h3"><a href="#2.-빈도주의(Frequentist)-VS-베이지안(Bayesian)">2. 빈도주의(Frequentist) VS 베이지안(Bayesian) </a>
<ul>
<li class="toc-entry toc-h4"><a href="#2.1.-빈도주의-확률-(Frequentist)">2.1. 빈도주의 확률 (Frequentist) </a></li>
<li class="toc-entry toc-h4"><a href="#2.2.-베이지안-확률-(Bayesian)">2.2. 베이지안 확률 (Bayesian) </a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#3.-베이즈-정리">3. 베이즈 정리 </a>
<ul>
<li class="toc-entry toc-h4"><a href="#3.1.-베이즈-정리의-직관적-이해-(1)">3.1. 베이즈 정리의 직관적 이해 (1) </a></li>
<li class="toc-entry toc-h4"><a href="#3.2.-베이즈-정리의-직관적-이해-(2)">3.2. 베이즈 정리의 직관적 이해 (2) </a></li>
<li class="toc-entry toc-h4"><a href="#3.3.-왜-머신러닝은-베이지안인가?">3.3. 왜 머신러닝은 베이지안인가? </a></li>
<li class="toc-entry toc-h4"><a href="#3.4.-베이즈-정리-속-각-부분의-의미">3.4. 베이즈 정리 속 각 부분의 의미 </a></li>
<li class="toc-entry toc-h4"><a href="#3.5.-베이즈-정리의-유도">3.5. 베이즈 정리의 유도 </a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#4.-Softmax와-Sigmoid-함수">4. Softmax와 Sigmoid 함수 </a>
<ul>
<li class="toc-entry toc-h4"><a href="#4.1.-Softmax-함수">4.1. Softmax 함수 </a></li>
<li class="toc-entry toc-h4"><a href="#4.2.-Sigmoid-함수">4.2. Sigmoid 함수 </a></li>
<li class="toc-entry toc-h4"><a href="#4.3.-어렵나요?-그래도-기억해야할-것!">4.3. 어렵나요? 그래도 기억해야할 것! </a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#5.-확률-분포">5. 확률 분포 </a>
<ul>
<li class="toc-entry toc-h4"><a href="#5.1.-이산확률분포-vs-연속확률분포">5.1. 이산확률분포 vs 연속확률분포 </a></li>
<li class="toc-entry toc-h4"><a href="#5.2.-균등확률분포(Uniform)-vs-정규확률분포(Normal-=-Gaussian)">5.2. 균등확률분포(Uniform) vs 정규확률분포(Normal = Gaussian) </a></li>
<li class="toc-entry toc-h4"><a href="#5.3-균등분포와-정규분포로-랜덤텐서-생성하기">5.3 균등분포와 정규분포로 랜덤텐서 생성하기 </a></li>
<li class="toc-entry toc-h4"><a href="#5.4-정규분포는-평균과-표준편차에-의해-그래프가-결정된다.">5.4 정규분포는 평균과 표준편차에 의해 그래프가 결정된다. </a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#6.-다양한-통계적-개념들">6. 다양한 통계적 개념들 </a>
<ul>
<li class="toc-entry toc-h4"><a href="#6.1.-평균-(Mean)">6.1. 평균 (Mean) </a></li>
<li class="toc-entry toc-h4"><a href="#6.2.-기댓값-(Expectation-Value)">6.2. 기댓값 (Expectation Value) </a></li>
<li class="toc-entry toc-h4"><a href="#6.3.-분산-(Variance)">6.3. 분산 (Variance) </a></li>
<li class="toc-entry toc-h4"><a href="#6.4.-표준편차">6.4. 표준편차 </a></li>
<li class="toc-entry toc-h4"><a href="#6.5-평균과-표준편차에-의한-정규분포의-변화">6.5 평균과 표준편차에 의한 정규분포의 변화 </a></li>
<li class="toc-entry toc-h4"><a href="#6.6.-표준화와-정규화">6.6. 표준화와 정규화 </a></li>
<li class="toc-entry toc-h4"><a href="#6.7-공분산과-상관계수">6.7 공분산과 상관계수 </a></li>
<li class="toc-entry toc-h4"><a href="#6.8-다중공선성">6.8 다중공선성 </a></li>
</ul>
</li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/02_03_통계학_기초.ipynb
-->

<div class="container" id="notebook-container">
        
    
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>본격적으로 머신러닝의 How에 해당하며, 데이터의 불확실성을 모델링 하는 방법인 통계에 대해 배워봅시다. 물론 통계 이론 전체를 배우려면 엄청나게 방대한 양의 내용을 배워야하지만, 시간이 한정적이므로 현재 머신러닝 모델들을 이해하는 수준까지만 공부하도록합니다.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br><br></p>
<h3 id="1.-통계학이란?">
<a class="anchor" href="#1.-%ED%86%B5%EA%B3%84%ED%95%99%EC%9D%B4%EB%9E%80?" aria-hidden="true"><span class="octicon octicon-link"></span></a>1. 통계학이란?<a class="anchor-link" href="#1.-%ED%86%B5%EA%B3%84%ED%95%99%EC%9D%B4%EB%9E%80?"> </a>
</h3>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day2/90.jpg?raw=true" alt="img"></p>
<p>통계란 무엇일까요? <strong>통계는 다양한 데이터를 가지고 다양한 가설을 세우는 과정</strong>입니다. 그러나 100% 정확한 가설은 거의 존재하지 않습니다. 때문에 우리는 이러한 가설의 불확실성에 대해 생각해봐야합니다. 통계에서는 이러한 <strong>불확실성을 '확률'이라는 도구를 사용하여 표현</strong>합니다. 그런데 문제는 이러한 <strong>'확률'을 바라보는 관점이 학파마다 달랐다</strong> 는 것입니다.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br></p>
<h3 id="2.-빈도주의(Frequentist)-VS-베이지안(Bayesian)">
<a class="anchor" href="#2.-%EB%B9%88%EB%8F%84%EC%A3%BC%EC%9D%98(Frequentist)-VS-%EB%B2%A0%EC%9D%B4%EC%A7%80%EC%95%88(Bayesian)" aria-hidden="true"><span class="octicon octicon-link"></span></a>2. 빈도주의(Frequentist) VS 베이지안(Bayesian)<a class="anchor-link" href="#2.-%EB%B9%88%EB%8F%84%EC%A3%BC%EC%9D%98(Frequentist)-VS-%EB%B2%A0%EC%9D%B4%EC%A7%80%EC%95%88(Bayesian)"> </a>
</h3>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day2/91.png?raw=true" alt="img"></p>
<p>확률을 바라보는 관점에는 크게 두가지가 있습니다. 첫번째는 빈도주의, 두번째는 베이지안입니다. 이들에 대해 알아봅시다.
<br><br></p>
<h4 id="2.1.-빈도주의-확률-(Frequentist)">
<a class="anchor" href="#2.1.-%EB%B9%88%EB%8F%84%EC%A3%BC%EC%9D%98-%ED%99%95%EB%A5%A0-(Frequentist)" aria-hidden="true"><span class="octicon octicon-link"></span></a>2.1. 빈도주의 확률 (Frequentist)<a class="anchor-link" href="#2.1.-%EB%B9%88%EB%8F%84%EC%A3%BC%EC%9D%98-%ED%99%95%EB%A5%A0-(Frequentist)"> </a>
</h4>
<p>빈도주의는 보통 <strong>데이터의 관점</strong>에서, 확률을 생각합니다. 이들 빈도주의자들에게는 확률이란 <strong>사건의 발생 빈도</strong>를 의미합니다. 간단하게 예를 들어 설명하겠습니다. 먼저 "동전을 던졌을 때 앞면이 나올 확률은 $0.5$이다." 라는 가설을 세웁니다. 이때 이 확률 $0.5$는 앞면이라는 <strong>사건이 발생할 확률</strong>을 의미합니다.
<br><br></p>
<p>그리고 나서 동전을 100번, 1000번 던져보고 앞면이 나올 확률이 약 $0.5$와 매우 비슷하다는 것을 확인하고 <strong>가설을 채택</strong>합니다. 즉, 빈도주의자들은 어떤 시스템을 모델링할 때, 가설(모델)을 고정해둔 상태에서 데이터를 계속 실험해봐서 가설이 맞는지 안맞는지 보고, 가설을 채택 혹은 기각합니다. 빈도주의 학파가 집중하는 불확실성은 Aleatory Uncertainty라고 하는데, 이는 <strong>사건 발생의 Random함이 불확실성을 만든다</strong>라는 관점으로 해석할 수 있습니다. 그들은 그들의 가설을 믿습니다. 그리고 데이터를 불신합니다.
<br><br></p>
<h4 id="2.2.-베이지안-확률-(Bayesian)">
<a class="anchor" href="#2.2.-%EB%B2%A0%EC%9D%B4%EC%A7%80%EC%95%88-%ED%99%95%EB%A5%A0-(Bayesian)" aria-hidden="true"><span class="octicon octicon-link"></span></a>2.2. 베이지안 확률 (Bayesian)<a class="anchor-link" href="#2.2.-%EB%B2%A0%EC%9D%B4%EC%A7%80%EC%95%88-%ED%99%95%EB%A5%A0-(Bayesian)"> </a>
</h4>
<p>베이지안은 보통 <strong>가설의 관점</strong>에서 확률을 생각합니다. 이들 베이지안들에게는 확률이란 <strong>가설의 신뢰도</strong>를 의미합니다. 간단하게 예를 들면, 먼저 "동전을 던졌을 때 앞면이 나올 확률이 절반 정도 일것이라는 가설의 신뢰도는 $0.5$이다"라고 생각합니다. 이 때, 확률 $0.5$는 <strong>주장하는 가설에 대한 신뢰도</strong>입니다. <br><br></p>
<p>그리고 나서 실험을 진행하고 앞면이 나올 확률이 절반정도 된다는 것을 확인하고 <strong>기존 가설의 신뢰도를 업데이트</strong>합니다. 계속해서 확인하면 계속 절반정도만 앞면이 나올 것이고 가설의 신뢰도는 거의 <strong>1.0</strong>에 가까워질 것입니다. 즉, 베이지안 학파는 어떤 시스템을 모델링할 때, 데이터를 고정한 상태에서 가설을 점점 잘 맞게 변화시켜나갑니다. 베이지안 학파가 집중하는 불확실성은 Epidemic Uncertainty라고 하는데, 이는 <strong>가설의 신뢰도가 불확실성을 만든다</strong>라는 관점으로 해석할 수 있습니다. 그들은 그들의 모델을 불신합니다. 그리고 데이터를 믿습니다. 그리고 이 것이 베이지안이 빈도주의를 꺾고 세상을 지배하게 된 이유입니다. 이러한 관점의 차이를 이해하고 베이즈 정리에 대해 이해해봅시다.<br><br></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br></p>
<h3 id="3.-베이즈-정리">
<a class="anchor" href="#3.-%EB%B2%A0%EC%9D%B4%EC%A6%88-%EC%A0%95%EB%A6%AC" aria-hidden="true"><span class="octicon octicon-link"></span></a>3. 베이즈 정리<a class="anchor-link" href="#3.-%EB%B2%A0%EC%9D%B4%EC%A6%88-%EC%A0%95%EB%A6%AC"> </a>
</h3>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day2/113.gif?raw=true" alt=""></p>
<p><br><br></p>
<h4 id="3.1.-베이즈-정리의-직관적-이해-(1)">
<a class="anchor" href="#3.1.-%EB%B2%A0%EC%9D%B4%EC%A6%88-%EC%A0%95%EB%A6%AC%EC%9D%98-%EC%A7%81%EA%B4%80%EC%A0%81-%EC%9D%B4%ED%95%B4-(1)" aria-hidden="true"><span class="octicon octicon-link"></span></a>3.1. 베이즈 정리의 직관적 이해 (1)<a class="anchor-link" href="#3.1.-%EB%B2%A0%EC%9D%B4%EC%A6%88-%EC%A0%95%EB%A6%AC%EC%9D%98-%EC%A7%81%EA%B4%80%EC%A0%81-%EC%9D%B4%ED%95%B4-(1)"> </a>
</h4>
<p>베이즈 정리의 수식을 곧바로 이해하는 것은 아직 어려울 것입니다. 따라서 직관적으로 이해할 수 있는 예제를 봅시다. 
<br><br></p>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day2/93.gif?raw=true" alt=""></p>
<p>Steve라는 미국인이 있었습니다. 그는 수줍지만 그래도 남들을 잘 도와주는 편이고 순하고 깨끗한 영혼을 가졌습니다. Steve의 성격이 주어졌을 때, 그는 농부일까요? 도서관 사서일까요?
<br><br></p>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day2/94.gif?raw=true" alt=""></p>
<p>대부분의 여러분이 농부보다는 도서관 사서와 어울린다고 생각하셨을 것입니다. 실제로 조사 결과 사서들의 대부분인 40%가 그러한 성격을 가지고 있었고, 농부들의 10%가 해당 성격을 가지고 있었다고 가정 해봅시다. 그렇다면 스티브는 사서일까요?<br><br></p>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day2/95.gif?raw=true" alt=""></p>
<p>그럴싸하지만 아닙니다. 미국에는 일반적으로 1 : 20의 비율로 농부의 수가 도서관 사서보다 훨씬 많습니다. 따라서 Steve는 농부일 수도 있습니다.  <br><br></p>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day2/96.gif?raw=true" alt=""></p>
<p>그렇다면 숫자를 10배 늘려서 직접 계산해봅시다. 아직 Steve의 성격이 무엇인지는 모르지만, Steve를 포함해서 미국에서 사서 10명과 농부 200명을 데려왔습니다. 왼쪽에 있는 10명의 사람은 도서관 사서이고, 오른쪽에 있는 200명의 사람은 농부입니다. Steve의 성격을 알기 전까지, Steve가 사서일거라는 주장의 신뢰도는 $\frac{10}{210} = 4.7$퍼센트가 됩니다.
<br><br></p>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day2/97.gif?raw=true" alt=""></p>
<p>그러다가, Steve가 해당 성격을 가졌다는 것을 알게 되었다고 합시다. 이 순간, Steve는 사서의 40%와 농부의 10%에 포함되게 되었습니다. 그래서 그 이외의 나머지 사람들(사서의 60%, 농부의 90%)은 의미가 없어졌습니다.<br><br></p>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day2/98.gif?raw=true" alt=""></p>
<p>Steve가 해당 성격을 가졌기 때문에, 해당 성격을 가지지 않은 나머지 사람들은 전부 제외하고, P(사서 확률 | 해당 성격을 가졌을 때)을 구합니다. Steve의 직업이 무엇이든, 성격이 그러한 것은 사실이므로 조건에 부합하지 않는 부분은 전부 제거합니다. <strong>이 부분이 베이즈 정리의 핵심입니다.</strong> 베이즈 정리는 결국 조건부확률이기 때문에 조건에 해당하지 않는 부분은 전부 제외할 수 있습니다. 따라서 표본공간이 변하게 되고, 계산 결과로 $\frac{4}{4 + 20} = 16.7$퍼센트가 나옵니다. 생각보다는 저조하죠? 그래도 성격에 대한 설명을 읽기 전인 $\frac{10}{210} = 4.7$퍼센트 보다는 높아졌습니다.
<br><br></p>
<p>Steve의 성격을 알게 된 것이 그 자체만으로 Steve가 사서일 것이라는 주장에 대한 신뢰도가 되는건 아닙니다. 베이지안 통계에서는 Epidemic Uncertainty를 다루기 때문에 데이터가 곧바로 어떤 분류일지에 대해 생각하는게 아니라, 그 데이터를 확인함으로서 기존 가설을 채택/기각하는 것이 아니라, 가설에 대한 신뢰도를 계속해서 업데이트 합니다. 
<br><br></p>
<h4 id="3.2.-베이즈-정리의-직관적-이해-(2)">
<a class="anchor" href="#3.2.-%EB%B2%A0%EC%9D%B4%EC%A6%88-%EC%A0%95%EB%A6%AC%EC%9D%98-%EC%A7%81%EA%B4%80%EC%A0%81-%EC%9D%B4%ED%95%B4-(2)" aria-hidden="true"><span class="octicon octicon-link"></span></a>3.2. 베이즈 정리의 직관적 이해 (2)<a class="anchor-link" href="#3.2.-%EB%B2%A0%EC%9D%B4%EC%A6%88-%EC%A0%95%EB%A6%AC%EC%9D%98-%EC%A7%81%EA%B4%80%EC%A0%81-%EC%9D%B4%ED%95%B4-(2)"> </a>
</h4>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day2/99.gif?raw=true" alt=""></p>
<p>또 다른 예시를 하나 봅시다. 수학자 W씨는 발렌타인 데이 때 좋아하던 여성분에게 초콜릿을 선물 받았습니다.
<br><br></p>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day2/100.gif?raw=true" alt=""></p>
<p>W씨는 머릿속에 온갖 생각이 다 들었지만, 여성분이 그냥 별뜻 없이 초콜릿을 준 것 일수도 있습니다. W씨는 수학을 이용해서 초콜릿을 받았을 때, 자신을 얼마나 좋아할지 계산해보기로 했습니다.
<br><br></p>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day2/101.gif?raw=true" alt=""></p>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day2/102.gif?raw=true" alt=""></p>
<p>직접 물어보기 전에는 상대가 얼만큼 자신을 좋아하는지 알 수 없기 때문에, 좋아하는 것과 싫어하는 것을 50대 50으로 가정하고, 계산을 하기로 했습니다. 50대 50으로 생각하는 것은 "이유 불충분의 원리"라고 부릅니다. 
<br><br></p>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day2/103.gif?raw=true" alt=""></p>
<p>직접 몇가지 통계자료를 조사해본 결과 좋아하는 사람에게 초콜릿을 줄 확률은 40%, 주지 않을 확률은 60%이며, 좋아하지 않는 사람에게 초콜릿을 줄 확률은 30%, 주지 않을 확률은 70%라는 것을 알았습니다.
<br><br></p>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day2/104.gif?raw=true" alt=""></p>
<p>예시를 들기 위해 100명의 사람이 있다고 해봅시다. 이유 불충분의 원리에 따라 50명의 사람은 누군가에게 사랑을 받고 있고, 나머지 50명은 받지 못하고 있습니다. 이 때, 사랑을 받고 있으면서 초콜릿을 줄 확률은 40%, 주지 않을 확률은 60% 입니다. 따라서 P(초콜릿 | 사랑받음) = 0.4입니다.
<br><br></p>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day2/105.gif?raw=true" alt=""></p>
<p>그리고, 예의상 초콜릿을 받은 사람 중에서 30%는 초콜릿을 받았고, 70%는 초콜릿을 받지 못했습니다. 따라서 P(초콜릿 | 사랑받지않음) = 0.3입니다.
<br><br></p>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day2/106.gif?raw=true" alt=""></p>
<p>우리가 원하는 것은 초콜릿을 받았을 때, 사랑받고 있을 확률입니다. 그러나 우리가 가진 데이터는 사랑받고 있을 때, 초콜릿을 가진 데이터입니다. 이 둘은 약간은 상반되는 개념입니다.
<br><br></p>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day2/107.gif?raw=true" alt=""></p>
<p>수학자 W씨는 이미 초콜릿을 받았고, 초콜릿을 받은 상황만 고려하면 됩니다. 따라서 조건에 해당되는 부분만 남기고 모두 지웁니다. 이 과정에서 표본공간이 변하게 됩니다. 
<br><br></p>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day2/108.gif?raw=true" alt=""></p>
<p>초콜릿을 받은 전체 35명의 사람 중, 20명이 사랑받고 있으니 P(사랑받고 있음 | 초콜릿)은 $\frac{20}{35}$로 총 57%가 됩니다.
<br><br></p>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day2/109.gif?raw=true" alt=""></p>
<p>따라서 수학자 W씨는 여성분이 자신을 좋아할 확률을 기존에 생각했던 50%에서 57%로 업데이트합니다. 그러나 57%는 너무 낮습니다. 수학자 W씨는 이대로 포기해야할까요?
<br><br></p>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day2/112.gif?raw=true" alt=""></p>
<p>그렇지 않습니다. 베이즈 정리는 데이터가 축적되면 축적될수록 훨씬 강해집니다. 수학자 W씨는 초콜릿을 받은 이후, 같이 밥을 먹은 사건, 밤새 전화를 했던 사건 등을 토대로 점점 사후확률을 더욱 높게 업데이트해 나갑니다. 그리고, 이러한 과정은 우리가 배우고 있는 머신러닝/딥러닝 알고리즘과 정확히 일치합니다.
<br><br></p>
<h4 id="3.3.-왜-머신러닝은-베이지안인가?">
<a class="anchor" href="#3.3.-%EC%99%9C-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%EC%9D%80-%EB%B2%A0%EC%9D%B4%EC%A7%80%EC%95%88%EC%9D%B8%EA%B0%80?" aria-hidden="true"><span class="octicon octicon-link"></span></a>3.3. 왜 머신러닝은 베이지안인가?<a class="anchor-link" href="#3.3.-%EC%99%9C-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%EC%9D%80-%EB%B2%A0%EC%9D%B4%EC%A7%80%EC%95%88%EC%9D%B8%EA%B0%80?"> </a>
</h4>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day2/110.png?raw=true" alt="">
<br></p>
<ul>
<li>머신러닝에 쓰이는 데이터셋은 고정적이다.</li>
</ul>
<p>빈도주의자들은 가설을 고정해놓고, 시행을 계속하며 랜덤한 데이터를 계속 만들어냅니다. 그러나 베이지안은 데이터를 고정해놓고, 시행을 계속하며 기존 가설을 점점 더 좋게 업데이트 합니다. 머신러닝에 있어서 데이터는 정말 귀중한 자원이며, 일정한 패턴을 띄고 있어야합니다. 따라서 무제한적으로 반복시행하여 만들어낸 랜덤 데이터 등이 머신러닝 데이터셋을 대체할 수 없습니다. 랜덤생성을 하지 않는다면 기존에 가진 데이터 셋으로 학습해야하는데, 무제한적으로 반복실험 할 만큼의 데이터를 구하는 것 자체가 너무나 어려운 일입니다.
<br><br></p>
<ul>
<li>머신러닝의 '학습'은 사실 사후확률 업데이트이다.</li>
</ul>
<p>몇몇 알고리즘(K-NN, 트리기반 등등)은 해당하지 않는 이야기이긴 합니다만, 그 외의 많은 머신러닝 분류 알고리즘과 모든 신경망 기반 딥러닝 분류 알고리즘들은 최종 출력값을 Sigmoid나 Softmax 함수 등에 연결하여 최종적인 사후확률을 구해냅니다. 이때 사용되는 <strong>Sigmoid 함수와 Softmax 함수가 바로 베이즈 정리 공식과 100% 동일한 함수</strong>입니다.
<br><br></p>
<p>즉, 대부분의 머신러닝/딥러닝 알고리즘의 출력값은 해당 데이터가 맞을 확률보다는, 해당 가설의 신뢰도를 의미합니다. 이렇게 구해진 <strong>사후확률은 다시 사전확률이 되고, 다시 한번 관측을 수행함으로서 더 나은 사후확률을 계속해서 만들어냅니다</strong>. 그래서 몇몇 전문가들은 머신러닝/딥러닝을 중/고급 통계응용이라고도 부릅니다.
<br><br></p>
<ul>
<li>머신러닝은 베이지안들의 철학, 그 자체이다.</li>
</ul>
<p>빈도주의자들은 가설(모델)을 아예 고정해놓고 데이터를 지속적으로 변화해서 실험한 뒤, 가설(모델)이 맞는 가설(모델)인지, 아닌지 평가합니다. 이러한 방식을 쓰면, 가설(모델)은 계속 고정되어있습니다. 만약 가설이 틀리면 사람이 직접 가설을 바꾸고 다시 테스트합니다. 따라서 가설 자체를 사람이 수동적으로 설정하고 데이터를 넣어보는 것이 빈도주의자들의 방식입니다. 이게 바로 Rule based 방식입니다. 규칙(if문)을 사람이 정합니다. 그리고 데이터를 넣어서 결과를 봅니다. 
<br><br></p>
<p>베이지안은 데이터셋을 고정해놓고, 가설(모델)을 지속적으로 변화시킵니다. 데이터를 설명할 분류/회귀를 위한 선이나 면(모델)을 계속 찾아나가면서 모델에 대한 신뢰도(정확도)를 계속 개선(업데이트)합니다. 이러한 방식을 쓰면, 데이터는 계속 고정되어있지만 모델이 계속 변합니다. 즉, 데이터는 고정하되, 모델이 계속 변하게 파라미터를 학습시키는 것이 베이지안들의 방식입니다. 그리고 이게 바로 머신러닝입니다. 규칙을 컴퓨터가 찾습니다. 대신 데이터는 고정되어있습니다. 
<br><br></p>
<p>제가 존경하는 조지워싱턴 대학의 최재화교수님은 베이지안을 부모 찾기라고 비유하셨습니다. (<a href="https://www.youtube.com/watch?v=Gpi6Uuw6DJM">https://www.youtube.com/watch?v=Gpi6Uuw6DJM</a>) 이제 막 태어난 많은 아이들이 있고 이들의 부모를 찾아야할때, 빈도주의자들은 부모입장에서 맞는 아이들을 매칭시키는가 하면, 베이지안은 아이들의 입장에서 부모들을 매칭시키는 것을 떠올리시면 정확합니다. 두 사상 모두 결국에 하고자 하는 것은 같지만 관점이 정 반대입니다.
<br><br></p>
<h4 id="3.4.-베이즈-정리-속-각-부분의-의미">
<a class="anchor" href="#3.4.-%EB%B2%A0%EC%9D%B4%EC%A6%88-%EC%A0%95%EB%A6%AC-%EC%86%8D-%EA%B0%81-%EB%B6%80%EB%B6%84%EC%9D%98-%EC%9D%98%EB%AF%B8" aria-hidden="true"><span class="octicon octicon-link"></span></a>3.4. 베이즈 정리 속 각 부분의 의미<a class="anchor-link" href="#3.4.-%EB%B2%A0%EC%9D%B4%EC%A6%88-%EC%A0%95%EB%A6%AC-%EC%86%8D-%EA%B0%81-%EB%B6%80%EB%B6%84%EC%9D%98-%EC%9D%98%EB%AF%B8"> </a>
</h4>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day2/92.jpg?raw=true" alt="">
<br><br></p>
<ul>
<li>Prior (사전확률) : $P(A)$</li>
</ul>
<p>Prior는 사전확률을 의미합니다. 사전확률은 데이터를 관측하기 전에 미리 예상하는 확률입니다. Steve의 예시에서는 미국의 농부 : 사서 = 20 : 1의 비율이 사전확률이였고, 발렌타인데이 예시는 이유 불충분의 원리로 사랑받음 : 사랑받지않음 = 50 : 50으로 설정한 것이 바로 사전확률입니다.
<br><br></p>
<ul>
<li>Likelihood (우도) : $P(B|A)$ </li>
</ul>
<p>베이즈 정리에서 가장 헷갈리는 영역인 Likelihood입니다. 그러나 어려울 것 하나도 없습니다. 예시를 생각해봅시다. Steve의 예시에서는 40%의 사서가 그러한 성격을 갖고 있다는 정보가 우도입니다. 발렌타인데이 예시에서는 보통 사랑하는 상태에서는 40%의 확률로 초콜릿을 준다는 정보가 Likelihood입니다. 우리가 구하고자 하는 확률은 $P(A|B)$입니다. 그러나 우도는 반대로 $P(B|A)$입니다. <br><br></p>
<p>이 것을 머신러닝의 문제로 확장해서 생각해봅시다. 분류 등의 문제를 풀 때, 우리가 알고 싶은 것은 $P(D|M)$입니다. 여기에서 D는 Data, M는 Model입니다. 우리가 머신러닝 모델을 사용, 테스트, 서비스 등을 할 때, 알고 싶은 것은 특정한 모델이 주어져서 고정되어 있을 때, 새로운 데이터를 잘 맞추는지에 대한 확률입니다. 이 것은 모델은 고정해둔 상태에서 데이터를 계속 변화시키는 빈도주의적 관점과 흡사하며, 이후에 말할 사후확률에 해당합니다.<br><br></p>
<p>그러나 우리가 <strong>학습</strong>할 때는 $P(M|D)$이 최대화되도록 학습시킵니다. (Maximum Likelihood Estimation) 우리는 모델을 학습시킬 때, 고정된 데이터셋을 사용합니다. 그리고 좋은 가설을 찾고자 합니다. Data가 고정되어 주어진 상태에서, 그 가설이 정답을 잘 맞추는 지에 대한 것이 Likelihood입니다. 이 것은 데이터를 고정한 상태에서 가설을 변화시키는 것이기 때문에 베이지안 관점과 흡사합니다. 머신러닝이 베이지안 그 자체라는 말이 어느정도는 느낌이 오시나요? 어디에서도 Likelihood를 이렇게 빈도주의와 베이지안의 철학 차이로 연결해서 설명하지 않습니다. (보통은 "확률분포함수의 함수값이다." 라는 설명을 합니다. 사실은 맞으나 와닿지는 않습니다)저는 이 내용을 깨닫는데 2년이 걸렸습니다. 여러분은 겨우 이틀만에 제 2년의 경험을 흡수하신 겁니다.<br><br></p>
<ul>
<li>Evidence : $P(B)$</li>
</ul>
<p>Evidence는 '증거'라고 불리는데, 사실 별로 와닿지 않아서 그냥 한국어 번역을 쓰지 않았습니다. Evidence는 $P(B)$입니다. 그러니까, 전체에서 $P(B)$라는 것은 필요 없는 것 다 지우고, 조건에 맞는 것만 보겠다라는 것입니다. 사실 표본공간을 $P(B)$로 제한하는 것이 베이지안 분석 (Bayesian Analysis)의 핵심 중 한가지인데, 머신러닝에서는 이 Evidence가 별로 중요하지 않습니다. 머신러닝에서는 $P(B)$는 $P(M)$에 해당하며, 현재 모델이 예측하려고 하는 모든 클래스를 이야기합니다. 만약 개와 고양이와 기린이을 분류하는 모델이 있다면 $P(M)$는 개 + 고양이 + 기린의 확률(=1)이며, $P(M|D)$(likelihood)는 데이터셋이 고정되어있을 때, 모델이 개 혹은 고양이 혹은 기린, 이들을 각각 맞출 확률을 이야기합니다.
<br><br></p>
<ul>
<li>Posterior (사후확률) : $P(B|A)$ </li>
</ul>
<p>사후확률은 위의 Likelihood 부분에서 말한대로 데이터를 모두 보고나서 평가되는 요소로 이전의 사전확률에 대비해 새롭게 업데이트 된 확률입니다. 머신러닝에서는 모델을 사용, 테스트, 서비스 등을 할 때를 이 사후확률의 개념이 정확도와 비슷하게 사용됩니다. 정확도는 학습을 마친 모델이 고정되어있을 때, 처음보는 새로운 유저의 데이터가 입력되었다면 그 데이터를 맞출 확률을 사후확률에 해당합니다. 개념이 확실히 잡히시나요?
<br><br></p>
<h4 id="3.5.-베이즈-정리의-유도">
<a class="anchor" href="#3.5.-%EB%B2%A0%EC%9D%B4%EC%A6%88-%EC%A0%95%EB%A6%AC%EC%9D%98-%EC%9C%A0%EB%8F%84" aria-hidden="true"><span class="octicon octicon-link"></span></a>3.5. 베이즈 정리의 유도<a class="anchor-link" href="#3.5.-%EB%B2%A0%EC%9D%B4%EC%A6%88-%EC%A0%95%EB%A6%AC%EC%9D%98-%EC%9C%A0%EB%8F%84"> </a>
</h4>
<p>(1) 우리는 이전 확률시간에 조건부 확률에 대해 배웠습니다. 베이즈정리는 조건부 확률과 동일합니다. 아래와 같은 식을 이용해 베이즈 정리의 공식을 유도할 수 있습니다.</p>
<ul>
<li>B가 일어날 때, A가 일어날 사건에 대한 조건부 확률은 $P(A|B) = \frac{P(A \cap B)}{P(B)}$입니다. </li>
<li>A가 일어날 때, B가 일어날 사건에 대한 조건부 확률은 $P(B|A) = \frac{P(B \cap A)}{P(A)}$입니다. 
<br><br>
</li>
</ul>
<p>(2) 두 식의 분모를 없게 하기 위해 분모를 곱한다면 아래와 같아집니다.</p>
<ul>
<li>$P(A|B) \cdot P(B) = P(A \cap B)$</li>
<li>$P(B|A) \cdot P(A) = P(B \cap A)$
<br><br>
</li>
</ul>
<p>(3) 이 때, $P(A \cap B)$와 $P(B \cap A)$는 동일합니다. 따라서, 아래와 같은 식의 유도가 가능해집니다.</p>
<ul>
<li>$P(A|B) \cdot P(B) = P(B|A) \cdot P(A)$ 
<br><br>
</li>
</ul>
<p>(4) 양 변을 $P(B)$로 나누면 베이즈 정리가 완성됩니다.</p>
<ul>
<li>$P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}$</li>
</ul>
<p><br><br></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="4.-Softmax와-Sigmoid-함수">
<a class="anchor" href="#4.-Softmax%EC%99%80-Sigmoid-%ED%95%A8%EC%88%98" aria-hidden="true"><span class="octicon octicon-link"></span></a>4. Softmax와 Sigmoid 함수<a class="anchor-link" href="#4.-Softmax%EC%99%80-Sigmoid-%ED%95%A8%EC%88%98"> </a>
</h3>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day2/117.png?raw=true" alt=""></p>
<p>이제 앞으로 머신러닝 모델들을 쭉 배우면서 계속 마주칠 Softmax함수와 Sigmoid함수입니다. 이들은 위에서 설명한 베이즈 정리와 정확히 동일한 함수입니다. 아래는 제가 직접 작성한 유도과정인데 상당히 어렵기 때문에 <strong>강의때는 다루지 않습니다.</strong> 만약 어떻게 저러한 모양의 함수가 작동하는지에 대해 궁금하시면 읽어보시고, 그게 아니라면 생략하셔도 무방합니다. 만약 읽다가 이해가 안가시면 저에게 질문해주세요! <br><br></p>
<p><strong>그러나 반드시 알아두셔야할 것은 Softmax와 Sigmoid라는 함수가 있고 앞으로 매번 나올 것인데, 이 함수들은 베이즈 정리와 동일하며, 확률값을 나타낸다는 것입니다.</strong>  <br><br></p>
<h4 id="4.1.-Softmax-함수">
<a class="anchor" href="#4.1.-Softmax-%ED%95%A8%EC%88%98" aria-hidden="true"><span class="octicon octicon-link"></span></a>4.1. Softmax 함수<a class="anchor-link" href="#4.1.-Softmax-%ED%95%A8%EC%88%98"> </a>
</h4>
<p>예를 들어 각각 강아지, 고양이라는 2개의 클래스가 있다고 해봅시다. 그렇다면, 우리가 원하는 것(Accuracy)는 모델이 주어졌을 때 강아지를 잘 맞출 확률 $P(Dog|Model)$, 모델이 주어졌을 때 고양이를 잘 맞출 확률입니다.$P(Cat|Model)$입니다. 강아지만 먼저 예시로 보겠습니다. 베이즈 정리로 풀어서 서술하면 아래와 같습니다.</p>
<p><br>

$$P(Dog|Model) = \frac{P(Model|Dog)P(Dog)}{P(Model)}$$

<br></p>
<p>이때, $P(Model)$은 모델이 현재 맞추려는 모든 클래스를 의미합니다. 모든 데이터셋은 겹치지 않기 때문에 (고양이이면서 강아지 일수 없음) 교집합이 없고, 그냥 덧셈으로 풀어서 쓸 수 있게 됩니다. 이를 전확률의 법칙이라고 합니다. 따라서 아래와 같이 식을 변경할 수 있습니다.</p>
<p><br>

$$P(Dog|Model) = \frac{P(Model|Dog)P(Dog)}{P(Model|Dog)P(Dog) + P(Model|Cat)P(Cat)}$$

<br></p>
<p>이 것을 함수로 만들고 싶은데, 확률은 0 ~ 1이기 때문에 음수 공역에서는 치역이 존재하지 않기 때문에 정의되지 않는 함수가 됩니다. 또한, Likelihood를 Log로 바꾸게 되면 독립일 때, 곱셈이 전부 덧셈으로 풀리게 되기 때문에 연산이 매우 편리합니다. 따라서 이들을 로그로 바꿔줍니다.</p>
<p><br>

$$P(Dog|Model) = \frac{\log P(Model|Dog)P(Dog)}{\log P(Model|Dog)P(Dog) + \log P(Model|Cat)P(Cat)}$$

<br></p>
<p>식이 너무 복잡하니 $\log P(Model|Dog)P(Dog) = z_{Dog}$으로 치환하고 $\log P(Model|Cat)P(Cat) = z_{Cat}$으로 치환하겠습니다. 그러면 아래와 같이 생각 할 수 있습니다.</p>
<p><br>

$$\log P(Model|Dog)P(Dog) = z_{Dog}$$


$$P(Model|Dog)P(Dog) = e^{z_{Dog}}$$

<br>

$$\log P(Model|Cat)P(Cat) = z_{Cat}$$


$$P(Model|Cat)P(Cat) = e^{z_{Cat}}$$

<br></p>
<p>기존의 확률값들을 자연상수 $e$의 지수꼴로 나타낼 수 있게 되었습니다. 따라서 이를 아래처럼 다시 정리합니다.</p>
<p><br>

$$P(Dog|Model) = \frac{e^{z_{Dog}}}{e^{z_{Dog}} + e^{z_{Cat}}}$$

<br></p>
<p>이를 시그마로 묶어서 더 깔끔하게 표현할 수 있고, 마찬가지로 고양이에 관한 사후확률도 표현이 가능합니다.</p>
<p><br>

$$P(Dog|Model) = \frac{e^{z_{Dog}}}{\sum_i e^{z_i}}$$

<br>

$$P(Cat|Model) = \frac{e^{z_{Cat}}}{\sum_i e^{z_i}}$$

<br></p>
<p>이러한 <strong>함수를 Softmax함수라고 부릅니다. Softmax함수는 보통 여러개의 클래스를 분류할 때 사용됩니다.</strong> 
<br><br></p>
<h4 id="4.2.-Sigmoid-함수">
<a class="anchor" href="#4.2.-Sigmoid-%ED%95%A8%EC%88%98" aria-hidden="true"><span class="octicon octicon-link"></span></a>4.2. Sigmoid 함수<a class="anchor-link" href="#4.2.-Sigmoid-%ED%95%A8%EC%88%98"> </a>
</h4>
<p>만약 클래스가 현재 예시처럼 2개 뿐이라면 한번 더 식을 정리 할 수 있습니다. 분모와 분자를 모두 $e^{z_{Dog}}$로 나누면 아래처럼 됩니다.</p>
<p><br>

$$P(Dog|Model) = \frac{e^{z_{Dog}}}{e^{z_{Dog}} + e^{z_{Cat}}}$$

<br>

$$P(Dog|Model) = \frac{\frac{e^{z_{Dog}}}{e^{z_{Dog}}}}{\frac{e^{z_{Dog}}}{e^{z_{Dog}}} + \frac{e^{z_{Cat}}}{e^{z_{Dog}}}}$$

<br>

$$P(Dog|Model) = \frac{1}{1 + \frac{e^{z_{Cat}}}{e^{z_{Dog}}}}$$

<br>

$$P(Dog|Model) = \frac{1}{1 + e^{z_{Cat} - z_{Dog}}}$$

<br>
<br></p>
<p>그리고 지수에 나온 뺄셈식($z_{Cat} - z_{Dog}$)을 풉니다.</p>
<p><br>

$$z_{Cat} - z_{Dog} = \log P(Model|Cat)P(Cat) - \log P(Model|Dog)P(Dog) $$

<br>

$$z_{Cat} - z_{Dog} = \log \frac{P(Model|Cat)P(Cat)}{P(Model|Dog)P(Dog)}$$

<br>

$$z_{Cat} - z_{Dog} = - \log \frac{P(Model|Dog)P(Dog)}{P(Model|Cat)P(Cat)}$$

<br>

$$z_{Cat} - z_{Dog} = - \log \frac{\frac{P(Model|Dog)P(Dog)}{P(Model)}}{\frac{P(Model|Cat)P(Cat)}{P(Model)}}$$

<br>

$$z_{Cat} - z_{Dog} = - \log \frac{P(Dog|Model)}{P(Cat|Model)}$$

<br>

$$z_{Cat} - z_{Dog} = - logit$$

<br></p>
<p>최종적으로 아래와 같은 식이 만들어집니다.</p>
<p><br>

$$P(Dog|Model) =  \frac{1}{1 + e^{z_{Cat} - z_{Dog}}} = \frac{1}{1 + e^{-logit}}$$

<br></p>
<p>이러한 <strong>함수를 Sigmoid함수라고 부릅니다. Sigmoid함수는 보통 2개의 클래스를 분류할 때 사용됩니다.</strong></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br><br></p>
<h4 id="4.3.-어렵나요?-그래도-기억해야할-것!">
<a class="anchor" href="#4.3.-%EC%96%B4%EB%A0%B5%EB%82%98%EC%9A%94?-%EA%B7%B8%EB%9E%98%EB%8F%84-%EA%B8%B0%EC%96%B5%ED%95%B4%EC%95%BC%ED%95%A0-%EA%B2%83!" aria-hidden="true"><span class="octicon octicon-link"></span></a>4.3. 어렵나요? 그래도 기억해야할 것!<a class="anchor-link" href="#4.3.-%EC%96%B4%EB%A0%B5%EB%82%98%EC%9A%94?-%EA%B7%B8%EB%9E%98%EB%8F%84-%EA%B8%B0%EC%96%B5%ED%95%B4%EC%95%BC%ED%95%A0-%EA%B2%83!"> </a>
</h4>
<p>위의 전개 내용은 궁금하신 분들만 확인하시고, 여러분들이 기억해야 할 내용은 아래와 같습니다.</p>
<ul>
<li>Sigmoid나 Softmax는 베이즈 정리와 동일하며, 이들은 <strong>확률</strong>을 나타냅니다.</li>
<li>두 함수는 사실 같은 함수이며, Softmax는 클래스가 여러개일 때, Sigmoid는 클래스가 2개일 때 씁니다.</li>
<li>그래프의 개형은 아래처럼 그려집니다. (Sigmoid 기준)
<br>
</li>
</ul>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
 
<span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">))</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mf">5.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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%0A">
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br><br></p>
<h3 id="5.-확률-분포">
<a class="anchor" href="#5.-%ED%99%95%EB%A5%A0-%EB%B6%84%ED%8F%AC" aria-hidden="true"><span class="octicon octicon-link"></span></a>5. 확률 분포<a class="anchor-link" href="#5.-%ED%99%95%EB%A5%A0-%EB%B6%84%ED%8F%AC"> </a>
</h3>
<p>확률 분포란 이름 그대로, 어떤 값에서 데이터들이 분포하고 있는지를 나타내는데요. 조금 더 자세히 정의를 해보자면 확률분포는 <strong>특정 값이 발생할 확률을 나타낸 함수</strong>입니다. 말이 어려운데 아래 예제를 볼까요? 
<br><br></p>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day2/115.png?raw=true" alt=""></p>
<p>이 함수는 생쥐의 무게를 모델링한 함수(분포)입니다. 이 때, $x$축은 생쥐의 몸무게라고 합시다.
<br><br></p>
<p><img src="https://github.com/gusdnd852/bigdata-lecture/blob/master/_notebooks/img/Day2/116.png?raw=true" alt=""></p>
<p>확률분포란 <strong>특정한 값을 가질 확률을 나타내는 함수</strong>입니다. 우리는 위의 그래프에서 생쥐의 몸무게가 32g에서 34g 사이일 확률은 빨간색 영역의 넓이와 같다고 '정의'할 수 있습니다. 이렇게 '<strong>특정 값(32g ~ 34g)</strong> 이 발생할 <strong>확률</strong>을'나타내는 것이 확률분포입니다. 어떻게 이러한 정의가 가능한 것일까요? 우리는 콜모고로프의 공리적 확률에 대해 배웠습니다. 최소 0, 최대 1이며, 배반사건들의 확률 합이 합사건의 확률과 같기만 하다면 위와 같이 확률을 정의할 수 있습니다. 이렇게 <strong>특정 값</strong>에서 사건이 발생할 확률을 모델링 한 것을 우리는 <strong>확률 분포</strong>라고 합니다. 즉, <strong>확률 분포는 특정 값에서의 확률을 모델링한 함수</strong>와 같습니다. <br><br></p>
<p>세상에는 정말 다양한 확률분포 함수가 존재합니다. 그러나 머신러닝에서는 그런 분포를 다 사용하지는 않습니다. 때문에 이 강의에서는 가장 기본적이고 필수적인 분포함수만 간단하게 짚고 넘어갑니다.
<br><br></p>
<h4 id="5.1.-이산확률분포-vs-연속확률분포">
<a class="anchor" href="#5.1.-%EC%9D%B4%EC%82%B0%ED%99%95%EB%A5%A0%EB%B6%84%ED%8F%AC-vs-%EC%97%B0%EC%86%8D%ED%99%95%EB%A5%A0%EB%B6%84%ED%8F%AC" aria-hidden="true"><span class="octicon octicon-link"></span></a>5.1. 이산확률분포 vs 연속확률분포<a class="anchor-link" href="#5.1.-%EC%9D%B4%EC%82%B0%ED%99%95%EB%A5%A0%EB%B6%84%ED%8F%AC-vs-%EC%97%B0%EC%86%8D%ED%99%95%EB%A5%A0%EB%B6%84%ED%8F%AC"> </a>
</h4>
<p><img src="https://img1.daumcdn.net/thumb/R800x0/?scode=mtistory2&amp;fname=https%3A%2F%2Ft1.daumcdn.net%2Fcfile%2Ftistory%2F99E192365D90B33B25" alt=""></p>
<ol>
<li>
<p><strong>이산확률분포</strong>는 관측치가 이산적일때 사용하는 확률분포입니다. <strong>이산적이라는 것은 연속과 반대의 개념인데, 정해진 값만 나올 수 있다고 생각하시면 됩니다.</strong> 쉽게 생각해서 디지털을 생각하시면 됩니다. 가령 주사위를 던질 때 1, 2, 3, 4, 5, 6의 값만 가능합니다. 주사위를 던져서 2.245가 나오거나 3.77이 나올 확률은 0입니다. 왜냐하면 주사위는 반드시 6개의 면만 나와야하기 때문입니다. 마찬가지로 동전을 던진다고 할 때도, 앞면(0)과 뒷면(1)만 나와야지 갑자기 0.45가 나온다거나 할 수 없습니다. 이런 사건들의 확률을 모델링할 때 사용하는 분포가 이산확률분포입니다. 
<br><br></p>
</li>
<li>
<p><strong>연속확률분포</strong>는 관측치가 연속적일 때 사용하는 확률분포입니다. <strong>연속적이라는 것은 이산과 반대 개념인데, 어떤 값이든 나올 수 있다고 생각하시면 됩니다.</strong> 쉽게 생각해서 아날로그를 생각하시면 됩니다. 가령 생쥐의 몸무게는 32g일수도 32.1g일수도 32.05g일수도 36g일수도 있습니다. 이렇게 관측치가 연속적인 사건들의 확률을 모델링할 때 사용하는 분포가 연속확률분포입니다.
<br><br></p>
</li>
</ol>
<h4 id="5.2.-균등확률분포(Uniform)-vs-정규확률분포(Normal-=-Gaussian)">
<a class="anchor" href="#5.2.-%EA%B7%A0%EB%93%B1%ED%99%95%EB%A5%A0%EB%B6%84%ED%8F%AC(Uniform)-vs-%EC%A0%95%EA%B7%9C%ED%99%95%EB%A5%A0%EB%B6%84%ED%8F%AC(Normal-=-Gaussian)" aria-hidden="true"><span class="octicon octicon-link"></span></a>5.2. 균등확률분포(Uniform) vs 정규확률분포(Normal = Gaussian)<a class="anchor-link" href="#5.2.-%EA%B7%A0%EB%93%B1%ED%99%95%EB%A5%A0%EB%B6%84%ED%8F%AC(Uniform)-vs-%EC%A0%95%EA%B7%9C%ED%99%95%EB%A5%A0%EB%B6%84%ED%8F%AC(Normal-=-Gaussian)"> </a>
</h4>
<p><img src="https://t1.daumcdn.net/cfile/tistory/9978FB455DB538731B" alt=""></p>
<ol>
<li>
<p><strong>균등확률분포</strong>는 <strong>모든 관측치가 동일한 확률로 발생할 때</strong> 사용하는 확률분포입니다. 예를 들어 주사위는 대표적인 균등분포 문제입니다. 우리가 주사위를 던질때, 6개의 면이 나올 확률이 모두 같습니다. 이런 사건들의 확률을 모델링할 때 사용하는 분포가 균등확률분포입니다. 이산/연속 확률분포와 균등/정규 확률분포는 서로 다른 개념이기 때문에 이산균등분포, 연속균등분포 등의 균등분포가 존재할 수 있습니다. 주사위는 당연히 이산균등분포겠죠?
<br><br></p>
</li>
<li>
<p><strong>정규확률분포</strong>는 <strong>관측치들이 동일하지 않은 확률로 발생할 때</strong> 사용하는 확률분포입니다. 가령 생쥐의 몸무게의 평균이 32g이라면 32g인 생쥐들은 비교적 많을 것이고 40g이나 28g인 생쥐들은 비교적 적을 것입니다. 이런 사건들의 확률을 모델링 할 때 사용하는 분포가 정규확률분포입니다. 이산/연속 확률분포와 균등/정규 확률분포는 서로 다른 개념이기 때문에 이산정규분포, 연속정규분포 등의 정규분포가 존재할 수 있습니다. 생쥐의 몸무게는 당연히 연속정규분포겠죠? 
<br><br></p>
</li>
</ol>
<h4 id="5.3-균등분포와-정규분포로-랜덤텐서-생성하기">
<a class="anchor" href="#5.3-%EA%B7%A0%EB%93%B1%EB%B6%84%ED%8F%AC%EC%99%80-%EC%A0%95%EA%B7%9C%EB%B6%84%ED%8F%AC%EB%A1%9C-%EB%9E%9C%EB%8D%A4%ED%85%90%EC%84%9C-%EC%83%9D%EC%84%B1%ED%95%98%EA%B8%B0" aria-hidden="true"><span class="octicon octicon-link"></span></a>5.3 균등분포와 정규분포로 랜덤텐서 생성하기<a class="anchor-link" href="#5.3-%EA%B7%A0%EB%93%B1%EB%B6%84%ED%8F%AC%EC%99%80-%EC%A0%95%EA%B7%9C%EB%B6%84%ED%8F%AC%EB%A1%9C-%EB%9E%9C%EB%8D%A4%ED%85%90%EC%84%9C-%EC%83%9D%EC%84%B1%ED%95%98%EA%B8%B0"> </a>
</h4>
<p>numpy에는 rand와 randn함수를 지원합니다. 우리는 이전에 np.random.rand만 배웠지만 randn함수도 존재합니다. 두 함수의 차이는 rand함수는 랜덤값을 균등분포로 생성하고, randn함수는 정규분포로 랜덤값을 생성합니다. 아래 예제를 봅시다.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">normal_dist</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10000</span><span class="p">)</span>
<span class="c1"># 정규분포로 랜덤값 생성하기 (평균 0, 표준편차 1)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">normal_dist</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">histtype</span><span class="o">=</span><span class="s1">'stepfilled'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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%0A">
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">normal_dist</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10000</span><span class="p">)</span>
<span class="c1"># 균등분포로 랜덤값 생성하기</span>

<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">normal_dist</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">histtype</span><span class="o">=</span><span class="s1">'stepfilled'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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%0A">
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br></p>
<h4 id="5.4-정규분포는-평균과-표준편차에-의해-그래프가-결정된다.">
<a class="anchor" href="#5.4-%EC%A0%95%EA%B7%9C%EB%B6%84%ED%8F%AC%EB%8A%94-%ED%8F%89%EA%B7%A0%EA%B3%BC-%ED%91%9C%EC%A4%80%ED%8E%B8%EC%B0%A8%EC%97%90-%EC%9D%98%ED%95%B4-%EA%B7%B8%EB%9E%98%ED%94%84%EA%B0%80-%EA%B2%B0%EC%A0%95%EB%90%9C%EB%8B%A4." aria-hidden="true"><span class="octicon octicon-link"></span></a>5.4 정규분포는 평균과 표준편차에 의해 그래프가 결정된다.<a class="anchor-link" href="#5.4-%EC%A0%95%EA%B7%9C%EB%B6%84%ED%8F%AC%EB%8A%94-%ED%8F%89%EA%B7%A0%EA%B3%BC-%ED%91%9C%EC%A4%80%ED%8E%B8%EC%B0%A8%EC%97%90-%EC%9D%98%ED%95%B4-%EA%B7%B8%EB%9E%98%ED%94%84%EA%B0%80-%EA%B2%B0%EC%A0%95%EB%90%9C%EB%8B%A4."> </a>
</h4>
<p>정규분포함수는 평균과 표준편차에 의해 그래프가 결정됩니다. 그런데 아직 평균과 표준편차가 무엇인지 배우지 않았습니다. 아래에서 이에 대해 자세히 알아봅시다.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">normal_dist_1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10000</span><span class="p">)</span>
<span class="c1"># 정규분포로 랜덤값 생성하기 (평균 0, 표준편차 1.0)</span>

<span class="n">normal_dist_2</span> <span class="o">=</span> <span class="mf">3.0</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10000</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1.5</span>
<span class="c1"># 정규분포로 랜덤값 생성하기 (평균 1.5, 표준편차 3.0)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">normal_dist_1</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">histtype</span><span class="o">=</span><span class="s1">'stepfilled'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">normal_dist_2</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">histtype</span><span class="o">=</span><span class="s1">'stepfilled'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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%0A">
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br></p>
<h3 id="6.-다양한-통계적-개념들">
<a class="anchor" href="#6.-%EB%8B%A4%EC%96%91%ED%95%9C-%ED%86%B5%EA%B3%84%EC%A0%81-%EA%B0%9C%EB%85%90%EB%93%A4" aria-hidden="true"><span class="octicon octicon-link"></span></a>6. 다양한 통계적 개념들<a class="anchor-link" href="#6.-%EB%8B%A4%EC%96%91%ED%95%9C-%ED%86%B5%EA%B3%84%EC%A0%81-%EA%B0%9C%EB%85%90%EB%93%A4"> </a>
</h3>
<p>여기에서는 평균, 기댓값, 분산, 표준편차 등 다양한 통계적 개념들에 대해 배웁니다. 
<br><br></p>
<h4 id="6.1.-평균-(Mean)">
<a class="anchor" href="#6.1.-%ED%8F%89%EA%B7%A0-(Mean)" aria-hidden="true"><span class="octicon octicon-link"></span></a>6.1. 평균 (Mean)<a class="anchor-link" href="#6.1.-%ED%8F%89%EA%B7%A0-(Mean)"> </a>
</h4>
<p><br>

$$mean = \frac{1}{N} \sum_{i} x_i$$

<br></p>
<p>평균은 어떤 자료가 n개 있을 때, 모든 자료의 값으로 더하고 개수인 n개로 나누면 그 것을 평균이라고 합니다. 평균을 일반적으로 해당 분포의 대표값으로 사용되는 값입니다. numpy에서는 mean()함수로 쉽게 구할 수 있습니다.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">vec_a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
<span class="n">vec_a</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>3.0</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br></p>
<h4 id="6.2.-기댓값-(Expectation-Value)">
<a class="anchor" href="#6.2.-%EA%B8%B0%EB%8C%93%EA%B0%92-(Expectation-Value)" aria-hidden="true"><span class="octicon octicon-link"></span></a>6.2. 기댓값 (Expectation Value)<a class="anchor-link" href="#6.2.-%EA%B8%B0%EB%8C%93%EA%B0%92-(Expectation-Value)"> </a>
</h4>
<p><br>

$$E[x] = \sum_{i} x_i \cdot P(x_i)$$

<br></p>
<p>기댓값은 그 사건이 일어날 확률과 값을 곱한 뒤 전부 더한 것입니다. 여기에서 확률은 위에서 언급한 <strong>확률분포</strong>에 기인합니다. 즉, 위에서 언급한 그냥 <strong>평균은 확률분포를 고려하지 않아서 모든 데이터를 균등분포라고 생각하고 계산</strong>하지만 기대값은 확률분포를 고려한 평균이라고 보시면 됩니다. <br><br></p>
<p>예를들어 10개의 데이터가 각각 발생할 확률이 다른데, 이들을 모두 더해서 평균을 내면 각 데이터의 값에 전부 같은 값인 $\frac{1}{10}$을 곱해서 $mean = \frac{1}{10} \sum_{i} x_i = \sum_{i} x_i \cdot \frac{1}{10}$처럼 계산합니다. 이는 $P(x_i)$를 모두 동일한 확률인 $\frac{1}{10}$으로 생각해서 계산한 것이라고 봐도 무방합니다. 즉, 평균의 경우 확률분포를 고려하지 않고 무조건 균등분포로 생각해서 계산합니다. <br><br></p>
<p>그러나 기대값의 경우는 $\frac{1}{N}$을 곱하는 것이 아니라 그 데이터가 발생할 확률인 $P(x_i)$을 곱해주기 때문에, 그 분포의 대표값을 설정할 때, 평균에 비해서더 적합한 대표값을 설정할 수 있습니다. (정규분포등의 분포 특성을 고려할 수 있음)</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br></p>
<h4 id="6.3.-분산-(Variance)">
<a class="anchor" href="#6.3.-%EB%B6%84%EC%82%B0-(Variance)" aria-hidden="true"><span class="octicon octicon-link"></span></a>6.3. 분산 (Variance)<a class="anchor-link" href="#6.3.-%EB%B6%84%EC%82%B0-(Variance)"> </a>
</h4>
<p><br>

$$deviation = x_i - mean$$

<br></p>
<p>분산 이전에 편차(Deviation)에 대해서 생각해봅시다. <strong>편차는 전체 데이터가 평균과 얼마나 차이가 나는지</strong>를 계산한 것으로, 각각 데이터의 값들에서 평균(기대값)을 뺀 값입니다. 만약 [1, 2, 3, 4, 5]라는 배열이 있으면 평균은 3입니다. 편차는 값에서 평균(기대값)을 뺀 것이므로 편차는 [3-1, 3-2, 3-3, 3-4, 3-5]이고, 계산하면 [-2, -1, 0, 1, 2]가 됩니다.
<br><br></p>
<p>
$$Variance = \frac{1}{N} \sum_{i} (x_i - mean)^2$$

<br></p>
<p>편차(deviation)는 실제 데이터가 평균과 얼마나 차이가 나는지 평가하기 위한 척도인데, 빼다보니 음수가 포함되어서 편차를 모두 더하니 $deviation = -2 -1 +0 + 1 + 2$로 $deviation = 0$이 되어버렸습니다. 그러나 실제로 편차가 0이려면 데이터는 [3, 3, 3, 3, 3]과 같아야합니다. 따라서 <strong>음수값을 없애주기 위해 편차값들에 모두 제곱을 해서 더해준 값이 분산</strong>입니다.<br><br></p>
<p>따라서 $Variance = \frac{1}{5} \cdot ((-2)^2 + (-1)^2 + 0^2 + 1^2 + 2^2) = \frac{1}{5} \cdot (4 + 1 + 0 + 1 + 4)$이고, 결과적으로 $Variance = \frac{1}{5} \cdot 10 = 2$가 되어 어느정도 편차를 잘 표현하고 있습니다. 즉, 정리하자면 분산은 데이터가 평균과 얼마나 차이나는지에 대한 척도입니다. numpy에서는 var()를 이용해 계산할 수 있습니다.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">vec_a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
<span class="n">vec_a</span><span class="o">.</span><span class="n">var</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>2.0</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br></p>
<h4 id="6.4.-표준편차">
<a class="anchor" href="#6.4.-%ED%91%9C%EC%A4%80%ED%8E%B8%EC%B0%A8" aria-hidden="true"><span class="octicon octicon-link"></span></a>6.4. 표준편차<a class="anchor-link" href="#6.4.-%ED%91%9C%EC%A4%80%ED%8E%B8%EC%B0%A8"> </a>
</h4>
<p><br>

$$Standard Deviation = \sqrt{\frac{1}{N} \sum_{i} (x_i - mean)^2}$$

<br></p>
<p>분산이 어느정도 편차를 잘 표현하긴 하지만 원래 값에 제곱을 해버린지라 실제적인 편차값과는 거리가 있습니다. 가령, 위의 예시에 데이터들의 편차는 2, 1, 0, 1, 2로 실제로 평균내면 $\frac{6}{5} = 1.2$정도 됩니다. 그러나 제곱을 해버려서 값이 $2$로 커졌습니다. 따라서 <strong>표준편차는 분산값에 다시 루트를 씌워서 제곱으로 인한 값의 차이를 줄입니다.</strong> 따라서 $\sqrt{Variance} = \sqrt{2} = 1.414$로 이전의 2보다는 조금더 실제 편차의 평균인 1.2에 가까워졌음을 알 수 있습니다. numpy에서는 std()함수를 사용해서 계산할 수 있습니다.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">vec_a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
<span class="n">vec_a</span><span class="o">.</span><span class="n">std</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>1.4142135623730951</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br></p>
<h4 id="6.5-평균과-표준편차에-의한-정규분포의-변화">
<a class="anchor" href="#6.5-%ED%8F%89%EA%B7%A0%EA%B3%BC-%ED%91%9C%EC%A4%80%ED%8E%B8%EC%B0%A8%EC%97%90-%EC%9D%98%ED%95%9C-%EC%A0%95%EA%B7%9C%EB%B6%84%ED%8F%AC%EC%9D%98-%EB%B3%80%ED%99%94" aria-hidden="true"><span class="octicon octicon-link"></span></a>6.5 평균과 표준편차에 의한 정규분포의 변화<a class="anchor-link" href="#6.5-%ED%8F%89%EA%B7%A0%EA%B3%BC-%ED%91%9C%EC%A4%80%ED%8E%B8%EC%B0%A8%EC%97%90-%EC%9D%98%ED%95%9C-%EC%A0%95%EA%B7%9C%EB%B6%84%ED%8F%AC%EC%9D%98-%EB%B3%80%ED%99%94"> </a>
</h4>
<p><img src="https://www.mathfactory.net/wp-content/uploads/%EC%88%98%ED%95%99-%EA%B3%B5%EC%8B%9D-%EC%A0%95%EA%B7%9C%EB%B6%84%ED%8F%AC-02.png" alt=""></p>
<p>정규분포 그래프는 평균과 표준편차에 의해 결정됩니다. 평균이 움직이면 그래프 자체가 옆으로 움직이고, 표준편차가 커지면 그래프가 보다 완만해지며(평균과 차이 큼), 표준편차가 작아지면 보다 뾰족해집니다. (평균과 차이 작음) 이를 numpy를 통해 확인할 수 있습니다.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">normal_dist_1</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10000</span><span class="p">)</span>
<span class="c1"># 정규분포로 랜덤값 생성하기 (평균 0, 표준편차 1)</span>

<span class="n">normal_dist_2</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10000</span><span class="p">)</span>
<span class="c1"># 정규분포로 랜덤값 생성하기 (평균 0, 표준편차 2)</span>

<span class="n">normal_dist_3</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10000</span><span class="p">)</span>
<span class="c1"># 정규분포로 랜덤값 생성하기 (평균 0, 표준편차 3)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">normal_dist_1</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'std = 1.0'</span> <span class="p">,</span><span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">histtype</span><span class="o">=</span><span class="s1">'stepfilled'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">normal_dist_2</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'std = 2.0'</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">histtype</span><span class="o">=</span><span class="s1">'stepfilled'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">normal_dist_3</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'std = 3.0'</span><span class="p">,</span><span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">histtype</span><span class="o">=</span><span class="s1">'stepfilled'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'std Difference'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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%0A">
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">normal_dist_1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10000</span><span class="p">)</span>
<span class="c1"># 정규분포로 랜덤값 생성하기 (평균 0, 표준편차 1.0)</span>

<span class="n">normal_dist_2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10000</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
<span class="c1"># 정규분포로 랜덤값 생성하기 (평균 0, 표준편차 1.0)</span>

<span class="n">normal_dist_3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10000</span><span class="p">)</span> <span class="o">+</span> <span class="mi">2</span>
<span class="c1"># 정규분포로 랜덤값 생성하기 (평균 0, 표준편차 1.0)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">normal_dist_1</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'mean = 0.0'</span> <span class="p">,</span><span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">histtype</span><span class="o">=</span><span class="s1">'stepfilled'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">normal_dist_2</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'mean = 1.0'</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">histtype</span><span class="o">=</span><span class="s1">'stepfilled'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">normal_dist_3</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'mean = 2.0'</span><span class="p">,</span><span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">histtype</span><span class="o">=</span><span class="s1">'stepfilled'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'mean Difference'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAbdUlEQVR4nO3de5RU5Z3u8e9DS2w1RuSiCTQRjqIYo/HSoC41QQkBNdG4NMfboEYcx+S0GR3l4MTLaMZcXEkmMeIcwhEDMXpQdCIsQyLKGScmHuUSwAmiBj0caTBKWi7xCq2/80dVt0VT3V3dXVW7atfzWauXvWu/VfutUp96+7ff/W5FBGZmVv36Jd0BMzMrDge6mVlKONDNzFLCgW5mlhIOdDOzlHCgm5mlhAPdLEvSNyXdlbN9lqT1kt6UdJSkQyStkPRXSd9Isq9m+cjz0K0WSHoCOA7YAQTwJ2Ae8KOIeK+T57wE/ENEzM9uzwK2RcTVZem0WQ95hG61pCki9gY+AVwDnAcslKRO2h8ArO5iu2CSduvN88x6woFuRSVpnaSpkp6V9JakWZL2l/TrbKnicUn75rQ/TtJTkrZIWiVpXM6+r0pak33ey5L+LmffOEnNkq6R9LqkVyV9tZA+RsRbEfEEcAZwPHB69jVvlvQLSbtLehOoA1ZJeknS/wZOBqZnSzAHZ9v9QNIrkl6TNEPSHh36N03Sn4GfSeon6brs67VIekDSwGz7EZJC0sXZ1/uLpOtz3m9dtiT0UvbzWC5peHbfaEmPSXpD0guS/mvv/u1ZtXOgWymcDUwADga+BPwa+CYwmMx/c98AkDQM+BVwKzAQuBZ4SNKQ7Ou8DnwR+BjwVeBHko7OOc7HgX2AYcAU4M7cL4vuRMQrwDLgpA6PvxcRH81ufiYiDoyIU4AnyYzyPxoRLwK3Zd/jkcBB2X7c1KF/A8mM7C/Pvu8vA58DhgKbgTs7dOtE4BBgPHCTpEOzj/8DcD5wWvbzuBR4W9JewGPAfcB+2Tb/KumwQj8HSw8HupXCHRHxWkRsIBOCz0TEimyt+pfAUdl2fwMsjIiFEfFBRDxGJmBPA4iIX0XES5HxH8Aidg7fHcC3ImJHRCwE3iQThj2xkUzo9ki2TPO3wNUR8UZE/BX4DpkyTpsPgH/KfkG8A/wdcH1ENGc/i5uBczqUY26JiHciYhWwCvhM9vHLgBsi4oXs57EqIlrIfOGti4ifRURrRPwBeAg4p6fvyaqf63pWCq/l/P5Onu220e8BwFckfSlnf3/g3wEknQr8E5lRcD9gT+A/c9q2RERrzvbbOa9dqGHAUz18DsCQbH+W55TgRaZM02ZTRLybs30A8EtJH+Q89j6wf872n3N+z30/w4GX8vTjAOBYSVtyHtsNuKfA92Ep4kC3JK0H7omIv+24Q9LuZEaaFwHzI2KHpIfJhGZRZGvQx5ApnfTUX8h8OR2W/Uskn45TyNYDl0bE7/P0ZUQ3x1sPHAj8Mc/j/xERE7rrsKWfSy6WpF8AX5I0MXvSrz57MrEB+AiwO7AJaM2O1r9QjINK2lPS54D5wBJgYU9fIyI+AP4nmbr+ftnXHSZpYhdPmwF8W9IB2fZDJJ1Z4CHvAv5Z0ihlHCFpEPAIcLCkyZL6Z3/G5NTerYY40C0xEbEeOJPMCdNNZEabU4F+2Zr0N4AHyJw8vABY0MdDTpf0VzIloB+T+QtgUjace2MasBZ4WtI24HG6ruHfTuY9LMr242ng2AKP9S9kPotFwDZgFrBH9nP6Apna/UYyJZvbyHwZWo3xhUVmZinhEbqZWUo40M3MUsKBbmaWEg50M7OUSGwe+uDBg2PEiBFJHd7MrCotX778LxExJN++xAJ9xIgRLFu2LKnDm5lVJUn/r7N9LrmYmaWEA93MLCUc6GZmKeHFucysSzt27KC5uZl33323+8ZWNPX19TQ0NNC/f/+Cn+NAN7MuNTc3s/feezNixAg6v1ufFVNE0NLSQnNzMyNHjiz4eS65mFmX3n33XQYNGuQwLyNJDBo0qMd/FTnQzaxbDvPy681n7kA3M0sJ19DNrEemzF5a1NebdcmYor5e0ubMmcOtt94KwA033MDFF1+8S5v33nuPiy66iOXLlzNo0CDuv/9+inHlvEfoZh3dd27mx6yH3njjDW655RaeeeYZlixZwi233MLmzZt3aTdr1iz23Xdf1q5dy9VXX820adOKcnwHulWNKbOXFn10aJVv3bp1jB49mssuu4xPf/rTXHjhhTz++OOccMIJjBo1iiVLlgDw1ltvcemllzJmzBiOOuoo5s+f3/78k046iaOPPpqjjz6ap57K3BP8iSeeYNy4cZxzzjmMHj2aCy+8kL7e8OfRRx9lwoQJDBw4kH333ZcJEybwm9/8Zpd28+fPbx+5n3POOSxevLjPxwaXXMysCqxdu5Z58+Yxc+ZMxowZw3333cfvfvc7FixYwHe+8x0efvhhvv3tb3PKKadw9913s2XLFsaOHcvnP/959ttvPx577DHq6+v505/+xPnnn9++jtSKFStYvXo1Q4cO5YQTTuD3v/89J5544k7H/v73v8+99967S58++9nP8pOf/GSnxzZs2MDw4cPbtxsaGtiwYdd7iOe222233dhnn31oaWlh8ODBffqcHOhmVvFGjhzJ4YcfDsBhhx3G+PHjkcThhx/OunXrAFi0aBELFizgBz/4AZCZbvnKK68wdOhQmpqaWLlyJXV1dbz44ovtrzt27FgaGhoAOPLII1m3bt0ugT516lSmTp1aUD/zjbLzzVYptF1POdDNrOLtvvuH97zu169f+3a/fv1obW0FMiH50EMPccghO9+n++abb2b//fdn1apVfPDBB9TX1+d93bq6uvbXytWTEXpDQwNPPPFE+3ZzczPjxo3b5bkNDQ2sX7+ehoYGWltb2bp1KwMHDuziEyiMa+hmlgoTJ07kjjvuaB/9rlixAoCtW7fyiU98gn79+nHPPffw/vvv9+h1p06dysqVK3f56RjmbX1YtGgRmzdvZvPmzSxatIiJEyfu0u6MM85gzpw5ADz44IOccsopHqGbdZR70jRt0+GaFje1/z59/PTE+lGpn+uNN97IVVddxRFHHEFEMGLECB555BG+/vWvc/bZZzNv3jxOPvlk9tprr5L1YeDAgdx4442MGZP5jG666ab2kfdNN91EY2MjZ5xxBlOmTGHy5MkcdNBBDBw4kLlz5xbl+CrGmdXeaGxsDN/gwnqiLay7CpSiBHrblMUL7u/d80skqUBfs2YNhx56aNmOZx/K99lLWh4Rjfnae4RuFc9TFc0K4xq6mVlKeIRu1pncq0UrrPxilo8D3SqSyyxmPeeSi5lZSjjQrap5fRezDxVUcpE0CbgdqAPuiojvddJuDPA0cG5EPFi0XppZ5Sj2SpQpOz8xadIknn76aU488UQeeeSRvG1KtXxut4EuqQ64E5gANANLJS2IiOfytLsNeLTPvbKa5dF271TKRUeWubL07bff5qc//WmnbXKXz507dy7Tpk3j/vv7/sVWSMllLLA2Il6OiO3AXODMPO2uBB4CXu9zr8zMsqpp+VyA8ePHs/fee3fZJsnlc4cB63O2m4FjcxtIGgacBZwCVOZ1wZZqHtmnW7Usn1uoJJfPzbdiTMevkh8D0yLi/a4WmJF0OXA5wCc/+clC+2hmNa5als8tVJLL5zYDw3O2G4CNHdo0AnOzHRoMnCapNSIezm0UETOBmZBZy6W3nTYrmmKs21Kha7+kSbUsn1uoUi2fW0igLwVGSRoJbADOAy7IbRARI9t+lzQbeKRjmJsVi8srlk/b8rl33HEHklixYgVHHXUUW7dupaGhgX79+jFnzpxeLZ9b7BF62/K5xx9/fHmXz42IVklNZGav1AF3R8RqSVdk98/ocy/MalzbLJWqmKFSoX+JVMLyuQAnnXQSzz//PG+++SYNDQ3MmjWLiRMnlmX53ILmoUfEQmBhh8fyBnlEXNL3bplZm9wpibVoxIgR/PGPf2zfnj17dt59e+yxR96pgqNGjeLZZ59t3/7ud78LwLhx43a6m9D06cX5Mn3yySfzPv6tb32r/ff6+nrmzZtXlOPl8louZsXixbwsYQ50s0IkcOKz1kfm1nNey8XMLCUc6GZmKeGSixkUf8Epdi2ZVMUMFqtqDnSzMvECWlZqDnQz65Fin6xN05fbypUr+drXvsa2bduoq6vj+uuv59xzd/3rL7Hlc80sRxVMTayqi5RSZs899+TnP/85o0aNYuPGjRxzzDFMnDiRAQMG7NQuyeVzzYrKdxmynqim5XMPPvhgRo0aBcDQoUPZb7/92LRp0y7tklw+18zy6epE6n3nQusrMOyY8vUnxapx+dwlS5awfft2DjzwwF32Jbl8rlm6lGBGS7nVWlml2pbPffXVV5k8eTJz5syhX79dCyFJLp9rZiXiq0ELU03L527bto3TTz+dW2+9leOOOy7v+0ly+Vyzksito8+6xDe6Agd8X1TC8rnbt2/nrLPO4qKLLuIrX/lKp+0SWz7XzIqvmoO7Uss8lbB87gMPPMBvf/tbWlpa2leFnD17NkceeWRZls9VMc6s9kZjY2O0nZiw2pJvhkvbCL2Ys186HfWXqYbeVIaTom3hmu8LoljBu2bNGg499NCivJb1TL7PXtLyiGjM194jdKsdSZwM3bD8w98948VKzIFuFcHz0s36zhcWmVm3kirN1rLefOYOdDPrUn19PS0tLQ71MooIWlpadppiWQiXXMysSw0NDTQ3N+e9hN1Kp76+vv2ip0I50C212urybbNdVq7f0r7vyOED8j6n2pRj+mP//v0ZOXJkyY9jfeeSi5lZSjjQzcxSwoFuZpYSDnSrSSvXb9mppm6WBj4paql25Ws3wH3pOAFq1h2P0M3MUsKBbmaWEg50M7OUcA3dysYLcJmVlkfoZinXtLipqm+oYYXzCN2sD5paXwFg+m6f3GnbLAkOdLNy800vrEQc6JYaV752Q4+f03ZxUVoW67La5kC3qtddkPuKUKsVPilqJTFl9lLPajErMwe6mVlKONDNzFLCNXSzlPLc89pT0Ahd0iRJL0haK+m6PPvPlPSspJWSlkk6sfhdNatcTa2vdD8HfcPynacsmhVZtyN0SXXAncAEoBlYKmlBRDyX02wxsCAiQtIRwAPA6FJ02MzM8itkhD4WWBsRL0fEdmAucGZug4h4MyIiu7kXEJiZWVkVEujDgPU5283Zx3Yi6SxJzwO/Ai7N90KSLs+WZJZt2rSpN/01M7NOFBLoyvPYLiPwiPhlRIwGvgz8c74XioiZEdEYEY1DhgzpWU/NzKxLhQR6MzA8Z7sB2NhZ44j4LXCgpMF97JuZmfVAIdMWlwKjJI0ENgDnARfkNpB0EPBS9qTo0cBHgJZid9asVHKXB/C6Llatug30iGiV1AQ8CtQBd0fEaklXZPfPAM4GLpK0A3gHODfnJKlZKuROS2xbLteskhR0YVFELAQWdnhsRs7vtwG3FbdrZlYqbRcdTR8/PeGeWDH5SlGzXvCNLKwSOdCtpLzioln5eHEuM7OUcKCbmaWEA93MLCVcQzerEfmW0/Vsl3TxCN3MLCUc6GZJ8hrpVkQuuVhReZqiWXI8QjczSwkHuplZSrjkYkXhUkt1y50B4xkv1csjdDOzlHCgm5mlhAPdzCwlHOhmnVi5fstOdzIyq3QOdDOzlPAsF+s1z2wpotyrRYcdk1w/rKo50M06cJnFqpUD3XrMI3OzyuRAN+uG7x9q1cKBblXlytduSLoLZhXLs1zMzFLCI3SzTvxwQAsA+9A/4Z6YFcYjdDOzlHCgm5mlhAPdrBr4VnVWANfQzXK01c0T1RbcvmLUesgjdDOzlHCgm5mlhAPdzHbStLhpp1vSWfVwDd2sB7a+swOAffYow9x0nwS1HnKgm3WjLcTNKp1LLmZmKeFANzNLCZdcrGLlrqx4x/63JtgTs+rgEbqZ5eXZLtXHgW5mlhIFBbqkSZJekLRW0nV59l8o6dnsz1OSPlP8rppZ+5ountJoeXRbQ5dUB9wJTACagaWSFkTEcznN/i/wuYjYLOlUYCZwbCk6bLXJdyoy614hI/SxwNqIeDkitgNzgTNzG0TEUxGxObv5NNBQ3G6amVl3Cgn0YcD6nO3m7GOdmQL8Ot8OSZdLWiZp2aZNmwrvpZmZdauQQFeexyJvQ+lkMoE+Ld/+iJgZEY0R0ThkyJDCe2lmZt0qZB56MzA8Z7sB2NixkaQjgLuAUyOiAhaVNjOrLYWM0JcCoySNlPQR4DxgQW4DSZ8E/g2YHBEvFr+bVm5TZi9lyuylSXej4m19Z0f7j1nSuh2hR0SrpCbgUaAOuDsiVku6Irt/BnATMAj4V0kArRHRWLpuW7nkhvqsS8Yk2BMz605Bl/5HxEJgYYfHZuT8fhlwWXG7ZmZmPeErRc3MUsKLc1nBXFM3q2weoZuZpYRH6Ga94FktVok8QjczSwmP0M2sILlro08fPz3BnlhnPEI3M0sJB7qZWUo40M2qnW94YVkOdDOzlPBJUasIviNR5fKNoquHR+hmZinhQDczSwkHutW0Hw5o4YcDins/Fq+PbklxDd2sSGopxNvq6r7AqLI40M2qlacqWgcOdKsZuaWVa7YMKvnxckfs++zRv+THM3MN3cwsJRzoZtZrTYubPE+9grjkYonyBUVmxeNAt534NnNm1cuBbgZFn4tulgTX0M3MUsIjdCs7183NSsMjdDOzlHCgm5mlhAPdzCwlXEO3snHtvMRy13YZdkxy/bDEONCtJnmaoqWRSy5mVlReDiA5HqGbrw61PnOAVwaP0M3MUsKBbpZGG5b7Bhg1yIFuZpYSDnSzMvCNo60cHOhmZinhQK9BU2YvrYmZLT8c0OL55lZTPG3RUsUBbrWsoBG6pEmSXpC0VtJ1efaPlvR/JL0n6drid9Os9zxSt1rR7QhdUh1wJzABaAaWSloQEc/lNHsD+Abw5ZL00syqTtvFRtPHT0+4J7WjkJLLWGBtRLwMIGkucCbQHugR8TrwuqTTS9JLM+sdL9hVUwopuQwD1udsN2cf6zFJl0taJmnZpk2bevMSZmbWiUICXXkei94cLCJmRkRjRDQOGTKkNy9hZr3lq0dTr5CSSzMwPGe7AdhYmu5YsbVNT5x1yZiEe2JA3ouL9tmjfwI9sTQqJNCXAqMkjQQ2AOcBF5S0V5YavqmFWfl0G+gR0SqpCXgUqAPujojVkq7I7p8h6ePAMuBjwAeSrgI+FRHbSth366NauLjIrJYUdGFRRCwEFnZ4bEbO738mU4oxM7OE+ErRGuHRuFn6eS0XM7OUcKCbmaWESy6WCl6rxcwjdDOz1HCgm5mlhEsuVhK+oKiCecGu1PII3axC+L6j1lceoZtZSbWti57La6SXhkfoZmYp4RG6FY3r5mbJcqBbVfP8c7MPOdBTJKn1WjwyN6sMDnSzhCU6s6VtCqOnL6aCA92qUq2UWtrC3nc1skI40K1q1EqI14K2qYyevlhcDnTrFdfNUyahq0cd7MXlQDerMLV4tWjuxUcO995zoFextlktsy4Zk3BPLFVyR+ttfNK0KjjQreK5dv6h3NG7T5RaR77038wsJRzoZmYp4UA3M0sJ19CrTFKX95uVi6cy9p4D3SqWT4aa9YxLLlaQK1+7wRcTmVU4j9BToBhlmNywvmP/WwtqV0wejVe4BOamu/TScw5065RH5GbVxSUXM6toTYub8t6X1HblEXqV8OyW2tbV+i5eYtfaONBtFy61mFUnB7olyidDq5jvdlRxXEOvIFNmL3Vpxcx6zSP0CpQb6qVeGjep8opH5n1Xa3V1r5nePQe6lY1DvLRq8cYYtjMHeoVLQwnGQZ6cNI7UgV2mMXrEnuFArwDlDu1ylVkc5JXDN8aoDQ70hJQrxAu9pN9qV5/DvqvZLl4yoKwc6GXi+39a6uULbyurggJd0iTgdqAOuCsivtdhv7L7TwPeBi6JiD8Uua9VI6nwLnUpJbeEcs2WQd22scpUSF292mrv+ZYGKGS5gLSN4rsNdEl1wJ3ABKAZWCppQUQ8l9PsVGBU9udY4H9k/2kdJHmSs6vAbwvi3KDO91ghz7PqkNismNyRfFv5pdDHyqCaSzaKiK4bSMcDN0fExOz2PwJExHdz2vwUeCIi/ld2+wVgXES82tnrNjY2xrJly3rV6XLN0y4kfGddMgbuOzfTfvu17Y9f+doNHDl8QPv2yvVb8taw84VsW7t89e/ejMI7hq5H0Vao3BF6xy+AfKP3ko3sCw3+zp6Xq7svig7nBPoa7Pn+UujLa0paHhGNefcVEOjnAJMi4rLs9mTg2IhoymnzCPC9iPhddnsxMC0ilnV4rcuBy7ObhwAv9O4t9chg4C9lOE4lqJX36veZLn6fPXNARAzJt6OQGrryPNbxW6CQNkTETGBmAccsGknLOvs2S5taea9+n+ni91k8hazl0gwMz9luADb2oo2ZmZVQIYG+FBglaaSkjwDnAQs6tFkAXKSM44CtXdXPzcys+LotuUREq6Qm4FEy0xbvjojVkq7I7p8BLCQzZXEtmWmLXy1dl3usrCWehNXKe/X7TBe/zyLp9qSomZlVB6+HbmaWEg50M7OUqKlAl3StpJA0OOm+lIKk70t6XtKzkn4paUD3z6oekiZJekHSWknXJd2fUpA0XNK/S1ojabWkv0+6T6UkqU7Siuy1LKklaYCkB7P/f67JXrBZdDUT6JKGk1m+4JWk+1JCjwGfjogjgBeBf0y4P0WTswTFqcCngPMlfSrZXpVEK3BNRBwKHAf8t5S+zzZ/D6xJuhNlcDvwm4gYDXyGEr3nmgl04EfAfyfPBU9pERGLIqI1u/k0mesB0mIssDYiXo6I7cBc4MyE+1R0EfFq28J2EfFXMv/jD0u2V6UhqQE4Hbgr6b6UkqSPAZ8FZgFExPaI2FKKY9VEoEs6A9gQEauS7ksZXQr8OulOFNEwYH3OdjMpDbo2kkYARwHPJNuTkvkxmUHWB0l3pMT+C7AJ+Fm2vHSXpL1KcaDUrIcu6XHg43l2XQ98E/hCeXtUGl29z4iYn21zPZk/3e8tZ99KrKDlJdJC0keBh4CrImJb0v0pNklfBF6PiOWSxiXdnxLbDTgauDIinpF0O3AdcGMpDpQKEfH5fI9LOhwYCazKLNtOA/AHSWMj4s9l7GJRdPY+20i6GPgiMD7SdZFBzSwvIak/mTC/NyL+Len+lMgJwBmSTgPqgY9J+kVE/E3C/SqFZqA5Itr+0nqQTKAXXc1dWCRpHdAYEalb3S17I5J/AT4XEZuS7k8xSdqNzIne8cAGMktSXBARqxPtWJFlbxYzB3gjIq5Kuj/lkB2hXxsRX0y6L6Ui6Ungsoh4QdLNwF4RMbXYx0nNCN0AmA7sDjyW/Wvk6Yi4ItkuFUdnS1Ak3K1SOAGYDPynpJXZx74ZEQsT7JP13ZXAvdn1sF6mRMuj1NwI3cwsrWpilouZWS1woJuZpYQD3cwsJRzoZmYp4UA3M0sJB7qZWUo40M3MUuL/Ax1GkR2RgufqAAAAAElFTkSuQmCC%0A">
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br></p>
<h4 id="6.6.-표준화와-정규화">
<a class="anchor" href="#6.6.-%ED%91%9C%EC%A4%80%ED%99%94%EC%99%80-%EC%A0%95%EA%B7%9C%ED%99%94" aria-hidden="true"><span class="octicon octicon-link"></span></a>6.6. 표준화와 정규화<a class="anchor-link" href="#6.6.-%ED%91%9C%EC%A4%80%ED%99%94%EC%99%80-%EC%A0%95%EA%B7%9C%ED%99%94"> </a>
</h4>
<p>표준화와 정규화는 머신러닝에서 빼놓을 수 없는 매우 중요한 과정입니다. 딥러닝 쪽에서 배치 정규화를 언급하면서 한번 더 자세하게 설명하겠지만, 통계파트에서도 설명합니다. <br><br></p>
<ol>
<li>표준화 (Standardization)</li>
</ol>
<p><br>

$$z = \frac{x - mean}{std}$$

<br></p>
<p>표준화 데이터의 분포를 <strong>평균 0으로, 분산 1에 가깝게 만듭니다.</strong> 표준화가 필요한 이유는 다음과 같습니다. 가령, 집의 크기와 가격을 모델링한 아래와 같은 데이터가 있다고 가정해봅시다.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#collapse-hide</span>

<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">housing_price_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">'넓이(m^2)'</span><span class="p">,</span> <span class="s1">'가격(원)'</span><span class="p">],</span>

    <span class="n">data</span><span class="o">=</span><span class="p">[[</span><span class="mi">35</span><span class="p">,</span> <span class="mi">10000000</span><span class="p">],</span>     
          <span class="p">[</span><span class="mi">40</span><span class="p">,</span> <span class="mi">12000000</span><span class="p">],</span>
          <span class="p">[</span><span class="mi">60</span><span class="p">,</span> <span class="mi">20000000</span><span class="p">],</span>
          <span class="p">[</span><span class="mi">60</span><span class="p">,</span> <span class="mi">20000000</span><span class="p">],</span>
          <span class="p">[</span><span class="mi">62</span><span class="p">,</span> <span class="mi">24000000</span><span class="p">],</span>
          <span class="p">[</span><span class="mi">60</span><span class="p">,</span> <span class="mi">22000000</span><span class="p">],</span>
          <span class="p">[</span><span class="mi">70</span><span class="p">,</span> <span class="mi">28000000</span><span class="p">],</span>
          <span class="p">[</span><span class="mi">80</span><span class="p">,</span> <span class="mi">32000000</span><span class="p">],</span>
          <span class="p">[</span><span class="mi">48</span><span class="p">,</span> <span class="mi">15000000</span><span class="p">],</span>
          <span class="p">[</span><span class="mi">60</span><span class="p">,</span> <span class="mi">23000000</span><span class="p">]]</span>
<span class="p">)</span>

<span class="n">housing_price_df</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>넓이(m^2)</th>
      <th>가격(원)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>35</td>
      <td>10000000</td>
    </tr>
    <tr>
      <th>1</th>
      <td>40</td>
      <td>12000000</td>
    </tr>
    <tr>
      <th>2</th>
      <td>60</td>
      <td>20000000</td>
    </tr>
    <tr>
      <th>3</th>
      <td>60</td>
      <td>20000000</td>
    </tr>
    <tr>
      <th>4</th>
      <td>62</td>
      <td>24000000</td>
    </tr>
    <tr>
      <th>5</th>
      <td>60</td>
      <td>22000000</td>
    </tr>
    <tr>
      <th>6</th>
      <td>70</td>
      <td>28000000</td>
    </tr>
    <tr>
      <th>7</th>
      <td>80</td>
      <td>32000000</td>
    </tr>
    <tr>
      <th>8</th>
      <td>48</td>
      <td>15000000</td>
    </tr>
    <tr>
      <th>9</th>
      <td>60</td>
      <td>23000000</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>이런 데이터는 컬럼별로 데이터의 크기가 너무 심하게 언밸런스 합니다. 이런 경우 학습시 한개의 컬럼에만 너무 큰 영향을 받을 수 있기 때문에, 표준화 작업을 진행해주는 것이 바람직합니다. 또한, 서로 다른 두가지 데이터셋을 비교할 때, 단위를 맞추는 용도로도 사용할 수 있으며, 조금더 쉽게 확률값을 구해낼 수 있습니다.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">housing_price_np</span> <span class="o">=</span> <span class="n">housing_price_df</span><span class="o">.</span><span class="n">values</span>

<span class="n">mean</span> <span class="o">=</span> <span class="n">housing_price_np</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">std</span> <span class="o">=</span> <span class="n">housing_price_np</span><span class="o">.</span><span class="n">std</span><span class="p">()</span>

<span class="n">standardization</span> <span class="o">=</span> <span class="p">(</span><span class="n">housing_price_np</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">/</span> <span class="n">std</span>
<span class="n">standardization_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">standardization</span><span class="p">,</span> 
                                  <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">'변환된 넓이(m^2)'</span><span class="p">,</span> <span class="s1">'변환된 가격(원)'</span><span class="p">])</span>
<span class="n">standardization_df</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>변환된 넓이(m^2)</th>
      <th>변환된 가격(원)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>-0.913224</td>
      <td>-0.026601</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-0.913224</td>
      <td>0.150724</td>
    </tr>
    <tr>
      <th>2</th>
      <td>-0.913222</td>
      <td>0.860025</td>
    </tr>
    <tr>
      <th>3</th>
      <td>-0.913222</td>
      <td>0.860025</td>
    </tr>
    <tr>
      <th>4</th>
      <td>-0.913222</td>
      <td>1.214675</td>
    </tr>
    <tr>
      <th>5</th>
      <td>-0.913222</td>
      <td>1.037350</td>
    </tr>
    <tr>
      <th>6</th>
      <td>-0.913221</td>
      <td>1.569325</td>
    </tr>
    <tr>
      <th>7</th>
      <td>-0.913220</td>
      <td>1.923976</td>
    </tr>
    <tr>
      <th>8</th>
      <td>-0.913223</td>
      <td>0.416712</td>
    </tr>
    <tr>
      <th>9</th>
      <td>-0.913222</td>
      <td>1.126012</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">std_mean</span> <span class="o">=</span> <span class="n">standardization</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">std_std</span> <span class="o">=</span> <span class="n">standardization</span><span class="o">.</span><span class="n">std</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">'변환 전 평균 : </span><span class="si">{}</span><span class="s1">, 표준편차 : </span><span class="si">{}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="n">std</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'변환 후 평균 : </span><span class="si">{}</span><span class="s1">, 표준편차 : </span><span class="si">{}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">std_mean</span><span class="p">,</span> <span class="n">std_std</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>변환 전 평균 : 10300028.75, 표준편차 : 11278714.809361352
변환 후 평균 : -2.2204460492503132e-17, 표준편차 : 1.0
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br></p>
<ol>
<li>정규화 (Min Max Scaling)</li>
</ol>
<p>정규화 기법은 매우 다양합니다. 그 중에서 Min Max Scaling 기법을 소개드리고 다른 기법들은 키워드로 제공해드리겠습니다. Min Max Scaling 기법은 데이터의 범위를 반드시 0 ~ 1로 만듭니다. <br><br></p>
<p><br>

$$minmax = \frac{x - x_{min}}{x_{max} - x_{min}}$$

<br></p>
<p>그러나 위처럼 데이터의 크기가 언밸런스 한 경우에 한쪽은 값이 0.9와 같이 크지만 한쪽은 값이 0.00001과 같이 작을 수 있습니다. 정규화를 하면 좋은 점은 눈으로 데이터를 확인하기 쉽고, 값을 마치 확률처럼 생각할 수 있다는 점이 장점입니다.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">housing_price_np</span> <span class="o">=</span> <span class="n">housing_price_df</span><span class="o">.</span><span class="n">values</span>

<span class="nb">min</span> <span class="o">=</span> <span class="n">housing_price_np</span><span class="o">.</span><span class="n">min</span><span class="p">()</span>
<span class="nb">max</span> <span class="o">=</span> <span class="n">housing_price_np</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>

<span class="n">minmax</span> <span class="o">=</span> <span class="p">(</span><span class="n">housing_price_np</span> <span class="o">-</span> <span class="nb">min</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="nb">max</span> <span class="o">-</span> <span class="nb">min</span><span class="p">)</span>
<span class="n">minmax_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">minmax</span><span class="p">,</span> 
                        <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">'변환된 넓이(m^2)'</span><span class="p">,</span> <span class="s1">'변환된 가격(원)'</span><span class="p">])</span>
<span class="n">minmax_df</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>변환된 넓이(m^2)</th>
      <th>변환된 가격(원)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.000000e+00</td>
      <td>0.312499</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1.562502e-07</td>
      <td>0.374999</td>
    </tr>
    <tr>
      <th>2</th>
      <td>7.812509e-07</td>
      <td>0.625000</td>
    </tr>
    <tr>
      <th>3</th>
      <td>7.812509e-07</td>
      <td>0.625000</td>
    </tr>
    <tr>
      <th>4</th>
      <td>8.437509e-07</td>
      <td>0.750000</td>
    </tr>
    <tr>
      <th>5</th>
      <td>7.812509e-07</td>
      <td>0.687500</td>
    </tr>
    <tr>
      <th>6</th>
      <td>1.093751e-06</td>
      <td>0.875000</td>
    </tr>
    <tr>
      <th>7</th>
      <td>1.406252e-06</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>8</th>
      <td>4.062504e-07</td>
      <td>0.468749</td>
    </tr>
    <tr>
      <th>9</th>
      <td>7.812509e-07</td>
      <td>0.718750</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>위에서 말한 것과 동일하게 범위가 0과 1사이로 좁혀지긴 했지만 두 데이터 간의 크기 차이가 분명하게 납니다. 그러나 0보다 작거나 1보다 큰 값은 없습니다. Min-Max 스케일링 말고도 절대값을 이용하는 MaxAbs Scaling, 이상치에 매우 강력한 Robust Scaling 등의 다양한 정규화 기법이 존재하고, sklearn 라이브러리를 이용하면 매우 쉽게 구현할 수 있습니다. 이에 대해서는 검색해보시면 더 자세한 결과를 얻을 수 있습니다.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br></p>
<h4 id="6.7-공분산과-상관계수">
<a class="anchor" href="#6.7-%EA%B3%B5%EB%B6%84%EC%82%B0%EA%B3%BC-%EC%83%81%EA%B4%80%EA%B3%84%EC%88%98" aria-hidden="true"><span class="octicon octicon-link"></span></a>6.7 공분산과 상관계수<a class="anchor-link" href="#6.7-%EA%B3%B5%EB%B6%84%EC%82%B0%EA%B3%BC-%EC%83%81%EA%B4%80%EA%B3%84%EC%88%98"> </a>
</h4>
<p><img src="https://t1.daumcdn.net/cfile/tistory/266F3149593F4D1A2C" alt=""></p>
<p>분산은 데이터와 평균이 얼마나 차이나느지에 관한 것이였습니다. 공분산은 두 개이상의 확률분포가 존재하고, 이 두 분포가 서로 어떤 관계를 가지고 있는지 계산하는 것입니다. 분산의 경우 $(x - mean)(x - mean)$의 평균이지만, 공분산은 $(x - mean_x)(y - mean_y)$의 평균입니다. 만약 공분산이 0보다 크면 양의 상관관계 (정비례) 관계가 있고, 만약 공분산이 0보다 작다면 음의 상관관계(반비례)관계가 됩니다. 식을 다시 정리해서 쓰면 아래와 같습니다.</p>
<p><br>

$$Covariance(X, Y) = \frac{1}{N} \sum_{i} (x_i - mean_x)(y_i - mean_y)$$

<br></p>
<p>그러나 공분산은 X와 Y의 단위에 크게 영향을 받습니다. X가 '키', Y가 '몸무게'라면 두 데이터의 단위가 다르고, 평균을 계산할때도 키의 평균과 몸무게의 평균은 차이가 있기 때문에 영향을 받을 수 밖에 없습니다.  이러한 문제를 해결하기 위해 상관계수라는 개념이 등장했습니다. 상관계수는 공분산을 정규화한 것으로 각각 데이터의 분산으로 나눠줘서 크기의 영향을 받지 않게 하였습니다. 상관계수의 정의는 아래와 같습니다.</p>
<p><br>

$$p = \frac{Cov(X, Y)}{\sqrt{Var(X)Var(Y)}}$$

<br></p>
<p><strong>상관관계가 1이라면 완전한 정비례, 상관관계가 -1이라면 완전한 반비례</strong>입니다. 상관관계는 반드시 -1 ~ 1 사이의 값을 가지며, 두 데이터분포가 얼마나 서로 연관있는지를 나타냅니다.
<br><br></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br><br></p>
<h4 id="6.8-다중공선성">
<a class="anchor" href="#6.8-%EB%8B%A4%EC%A4%91%EA%B3%B5%EC%84%A0%EC%84%B1" aria-hidden="true"><span class="octicon octicon-link"></span></a>6.8 다중공선성<a class="anchor-link" href="#6.8-%EB%8B%A4%EC%A4%91%EA%B3%B5%EC%84%A0%EC%84%B1"> </a>
</h4>
<p>일반적으로 우리가 회귀분석을 할 때, 정답과 데이터간의 상관관계를 봅니다. 만약 키(데이터)가 어느정도 크면 몸무게(정답)도 어느정도 나간다. 라고 생각하고 모델링할 수 있습니다. 그러나 데이터-정답 간의 상관관계가 아니라 데이터-데이터 간의 상관관계가 나타나는 경우가 있습니다. 이를 다중공선성이라고 합니다. 다중공선성이 있으면 회귀분석의 결과가 매우 불안정해집니다. 따라서 여러개의 분포를 다루는 경우 상관관계를 꼭 분석해보고 다중공선성이 있는지 확인해봐야합니다.
<br><br></p>
<p>통계는 여기까지 진행하고 다음시간에는 정보이론을 배우도록 하겠습니다.</p>

</div>
</div>
</div>
</div>





    <br><br>
    <div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = “https://gusdnd852.github.io/statics_theory“;
this.page.identifier = 03. 통계학 기초;
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://gusdnd852.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

<a class="u-url" href="/bigdata-lecture/statics_theory" hidden></a>
</article>

<script>
	function run_exec(){
		if("colab" === "binder"){
			if(confirm("이 노트북은 CPU환경인 Binder에서 소스코드를 실행합니다.")){
				var binder_path = "https://mybinder.org/v2/gh/gusdnd852/bigdata-lecture/master?filepath=_notebooks%2F02_03_%ED%86%B5%EA%B3%84%ED%95%99_%EA%B8%B0%EC%B4%88.ipynb";
				alert("Binder는 Docker기반의 컨테이너를 직접 빌드하기 때문에 시간이 다소 소요됩니다. Build logs의 show버튼을 눌러서 진행상황을 확인하세요");
				window.open(binder_path, "_blank");
							
			}
			
		}else if("colab" === "colab"){
			if(confirm("이 노트북은 GPU환경인 Google Colab에서 소스코드를 실행합니다.")){
				var colab_path = "https://colab.research.google.com/github/gusdnd852/bigdata-lecture/blob/master/_notebooks/02_03_통계학_기초.ipynb";
				alert("GPU 설정을 위해 Colab 상단 메뉴중, 런타임 → 런타임 유형변경에서 GPU를 선택해주세요. (미선택시 CPU에서 실행됨)");
				window.open(colab_path, "_blank");
			}			
		}else{
			alert("해당 실행환경은 구동이 불가능합니다. Binder와 Google Colab 중 한가지를 선택해주세요.");
		}
	}

    function code_listener() {
	var input_area = document.getElementsByClassName("input_area");

	for(var i = 0 ; i < input_area.length; i++){
		input_area[i].onclick = function(){
			run_exec();
    		}
	}

    } 

    code_listener();
</script>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/bigdata-lecture/"></data>

  <div class="wrapper">
      <div class="footer-col">
	<footer-elem class=px-2>
		<a href="https://github.com/gusdnd852">
			<svg class="social svg-icon">
				<use xlink:href="/bigdata-lecture/assets/minima-social-icons.svg#github"></use>
			</svg>
			<span class="username">Github</span>
		</a>
	</footer-elem>

	<footer-elem class=px-2>
		<a href="https://youtube.com/channel/UCb7DJAuj1LulbY8WEr2EDUw">
			<svg class="social svg-icon">
				<use xlink:href="/bigdata-lecture/assets/minima-social-icons.svg#youtube"></use>
			</svg>
			<span class="username">Youtube</span>
		</a>
	</footer-elem>

	<footer-elem class=px-2>
		<a href="https://www.facebook.com/gusdnd852">
			<svg class="social svg-icon">
				<use xlink:href="/bigdata-lecture/assets/minima-social-icons.svg#facebook"></use>
			</svg>
			<span class="username">Facebook</span>
		</a>
	</footer-elem>
	<br>
	<div class="copyright">
		© Copyright 2020 Hyunwoong Go. All rights reserved.
	</div>
      </div>
  </div>
</footer>
</body>
</html>
